//
// auto-generated by op2.py
//

void update_omp4_kernel(float *data0, int dat0size, float *data1, int dat1size,
                        float *data2, int dat2size, float *arg3, float *arg4,
                        int count, int num_teams, int nthread,
                        int direct_update_stride_OP2CONSTANT) {

  float arg3_l = *arg3;
  float arg4_l = *arg4;
  #pragma omp target teams num_teams(num_teams) thread_limit(nthread) map(to:data0[0:dat0size],data1[0:dat1size],data2[0:dat2size]) \
    map(to: alpha_ompkernel)\
    map(tofrom: arg3_l, arg4_l) reduction(+:arg3_l) reduction(max:arg4_l)
  #pragma omp distribute parallel for schedule(static,1) reduction(+:arg3_l) reduction(max:arg4_l)
  for ( int n_op=0; n_op<count; n_op++ ){
    //variable mapping
    const float *r = &data0[n_op];
    float *du = &data1[n_op];
    float *u = &data2[n_op];
    float *u_sum = &arg3_l;
    float *u_max = &arg4_l;

    //inline function

    u[(0) * direct_update_stride_OP2CONSTANT] +=
        du[(0) * direct_update_stride_OP2CONSTANT] +
        alpha_ompkernel * (r[(0) * direct_update_stride_OP2CONSTANT]);
    du[(0) * direct_update_stride_OP2CONSTANT] = 0.0f;
    *u_sum += (u[(0) * direct_update_stride_OP2CONSTANT]) *
              (u[(0) * direct_update_stride_OP2CONSTANT]);
    *u_max = MAX(*u_max, u[(0) * direct_update_stride_OP2CONSTANT]);
    //end inline func
  }

  *arg3 = arg3_l;
  *arg4 = arg4_l;
}
