//
// auto-generated by op2.py
//

//user function
#include "../res_calc1.h"
#include "op_lib_mpi.h"

// host stub function
void op_par_loop_res_calc1(char const *name, op_set set,
  op_arg arg0,
  op_arg arg1,
  op_arg arg2,
  op_arg arg3){

  int nargs = 4;
  op_arg args[4];

  args[0] = arg0;
  args[1] = arg1;
  args[2] = arg2;
  args[3] = arg3;

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timing_realloc(0);
  op_timers_core(&cpu_t1, &wall_t1);

  if (OP_diags>2) {
    printf(" kernel routine with indirection: res_calc1\n");
  }

  int my_rank=0;
  op_rank(&my_rank);

#ifdef COMM_AVOID
  int exec_size = 0;
  // for(int l = 0; l < 2; l++){
  //   exec_size += OP_aug_import_exec_lists[l][set->index]->size;
  // }
  // set_size = set->size + exec_size;
  int set_size = op_mpi_halo_exchanges_chained(set, nargs, args, 1);
  printf("res_calc1 aug_maps COMM_AVOID my_rank=%d set=%s set->size=%d, set_size=%d\n", my_rank, set->name, set->size, set_size);
#else
  
  int set_size = op_mpi_halo_exchanges(set, nargs, args);
  printf("res_calc1 aug_maps OP2 my_rank=%d set=%s set_size=%d\n", my_rank, set->name, set_size);
#endif
  if (set_size > 0) {

    for ( int n=0; n<set_size; n++ ){
      if (n==set->core_size) {
        op_mpi_wait_all(nargs, args);
      }
      int map0idx;
      int map1idx;
      
      #ifdef COMM_AVOID
      map0idx = arg0.map->aug_maps[1][n * arg0.map->dim + 0];
      map1idx = arg0.map->aug_maps[1][n * arg0.map->dim + 1];
      printf("res_calc1 aug_maps my_rank=%d map=%s n=%d map0[%d]=%d map1[%d]=%d\n", my_rank, arg0.map->name, n, n * arg0.map->dim + 0, map0idx, n * arg0.map->dim + 1, map1idx);
      #else
      map0idx = arg0.map_data[n * arg0.map->dim + 0];
      map1idx = arg0.map_data[n * arg0.map->dim + 1];
      #endif

      res_calc1(
        &((double*)arg0.data)[1 * map0idx],
        &((double*)arg0.data)[1 * map1idx],
        &((double*)arg2.data)[1 * map0idx],
        &((double*)arg2.data)[1 * map1idx]);
    }
  }

  if (set_size == 0 || set_size == set->core_size) {
    op_mpi_wait_all(nargs, args);
  }

#ifndef COMM_AVOID
  // combine reduction data
  op_mpi_set_dirtybit(nargs, args);
#endif
  // update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  OP_kernels[0].name      = name;
  OP_kernels[0].count    += 1;
  OP_kernels[0].time     += wall_t2 - wall_t1;
  OP_kernels[0].transfer += (float)set->size * arg0.size;
  OP_kernels[0].transfer += (float)set->size * arg2.size * 2.0f;
  OP_kernels[0].transfer += (float)set->size * arg0.map->dim * 4.0f;
}
