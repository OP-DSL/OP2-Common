#
# The following environment variables should be predefined:
#
# CUDA_INSTALL_PATH
# PARMETIS_INSTALL_PATH
# PTSCOTCH_INSTALL_PATH
# HDF5_INSTALL_PATH
#
# OP2_INSTALL_PATH
# OP2_COMPILER (gnu,intel,etc)
#

#
# set paths for header files and libraries
#
OP2_INC		= -I$(OP2_INSTALL_PATH)/c/include
OP2_LIB		= -L$(OP2_INSTALL_PATH)/c/lib

CUDA_INC	= -I$(CUDA_INSTALL_PATH)/include
CUDA_LIB	= -L$(CUDA_INSTALL_PATH)/lib64


ifeq ($(OP2_COMPILER),gnu)
  CPP		= g++
  CPPFLAGS	= -O2 -fPIC -DUNIX -Wall -O0 -g -Wextra
  OMPFLAGS	= -fopenmp
  MPICPP	= $(MPI_INSTALL_PATH)/bin/mpiCC
  MPIFLAGS	= $(CCFLAGS)
else
ifeq ($(OP2_COMPILER),intel)
  CPP		= icpc
  CCFLAGS	= -O3 -xAVX -DMPICH_IGNORE_CXX_SEEK -restrict -fno-alias -inline-forceinline -qopt-report=5 -parallel -DVECTORIZE #-parallel #-DCOMM_PERF #-DDEBUG #-vec-report
#  CCFLAGS	= -O3 -xAVX -DMPICH_IGNORE_CXX_SEEK -fno-alias -inline-forceinline -qopt-report -parallel -prec-div -DVECTORIZE #-parallel #-DCOMM_PERF #-DDEBUG #-vec-report
  CPPFLAGS 	= $(CCFLAGS)
  OMPFLAGS	= -openmp -openmp-report2
  MPICPP	= $(MPI_INSTALL_PATH)/bin/mpicxx
  MPIFLAGS	= $(CPPFLAGS)
else
ifeq ($(OP2_COMPILER),xl)
  CPP		= xlc++
  CCFLAGS	= -O5 -qarch=pwr8 -qtune=pwr8 -qhot
#  CCFLAGS	= -O3 -xAVX -DMPICH_IGNORE_CXX_SEEK -fno-alias -inline-forceinline -qopt-report -parallel -prec-div -DVECTORIZE #-parallel #-DCOMM_PERF #-DDEBUG #-vec-report
  CPPFLAGS 	= $(CCFLAGS)
  OMPFLAGS	= -qsmp=omp -qthreaded
  MPICPP	= $(MPI_INSTALL_PATH)/bin/mpicxx
  MPIFLAGS	= $(CPPFLAGS)
else
ifeq ($(OP2_COMPILER),pgi)
  CPP       	= pgc++
  CCFLAGS  	= -O3
  CPPFLAGS 	= $(CCFLAGS)
  OMPFLAGS 	= -mp
  MPICC   	= $(MPI_INSTALL_PATH)/bin/mpicc
  MPICPP   	= $(MPI_INSTALL_PATH)/bin/mpicxx
  MPIFLAGS 	= $(CPPFLAGS)
  NVCCFLAGS	= -ccbin=$(MPICPP)
  ACCFLAGS      = -acc -Minfo=acc -ta=tesla:cc35 -DOPENACC
else
ifeq ($(OP2_COMPILER),cray)
  CPP           = CC
  CCFLAGS       = -O3 -h fp3 -h ipa5
  CPPFLAGS      = $(CCFLAGS)
  OMPFLAGS      = -h omp
  MPICPP        = CC
  MPIFLAGS      = $(CPPFLAGS)
else
print:
	@echo "unrecognised value for OP2_COMPILER"
endif
endif
endif
endif
endif



#
# set flags for NVCC compilation and linking
#
ifndef NV_ARCH
  MESSAGE=select an NVIDA device to compile in CUDA, e.g. make NV_ARCH=KEPLER
  NV_ARCH=Kepler
endif
ifeq ($(NV_ARCH),Fermi)
  CODE_GEN_CUDA=-gencode arch=compute_20,code=sm_21
else
ifeq ($(NV_ARCH),Kepler)
  CODE_GEN_CUDA=-gencode arch=compute_35,code=sm_35
endif
endif

NVCCFLAGS       := $(NVCCFLAGS) $(CODE_GEN_CUDA) -m64 -Xptxas -dlcm=ca -Xptxas=-v -use_fast_math -O3 #-g -G -O0

VAR		= #-DOP_PART_SIZE_1=160 -DOP_PART_SIZE_2=320 -DOP_PART_SIZE_3=64 #-DOP_BLOCK_SIZE_0=64 -DOP_BLOCK_SIZE_1=64 -DOP_BLOCK_SIZE_2=64 -DOP_BLOCK_SIZE_3=64 -DOP_BLOCK_SIZE_4=64

#
# partitioning software for MPI versions
#
PARMETIS_VER=4
ifeq ($(PARMETIS_VER),4)
  PARMETIS_INC = -I$(PARMETIS_INSTALL_PATH)/include -DHAVE_PARMETIS -DPARMETIS_VER_4
  PARMETIS_LIB = -L$(PARMETIS_INSTALL_PATH)/lib -lparmetis -lmetis
else
  PARMETIS_INC = -I$(PARMETIS_INSTALL_PATH)/ -DHAVE_PARMETIS
  PARMETIS_LIB = -L$(PARMETIS_INSTALL_PATH)/ -lparmetis -lmetis
endif

PTSCOTCH_INC 	= -I$(PTSCOTCH_INSTALL_PATH)/include -DHAVE_PTSCOTCH
PTSCOTCH_LIB 	= -L$(PTSCOTCH_INSTALL_PATH)/lib/ -lptscotch \
                  -L$(PTSCOTCH_INSTALL_PATH)/lib/ -lptscotcherr

HDF5_INC = -I$(HDF5_INSTALL_PATH)/include
HDF5_LIB = -L$(HDF5_INSTALL_PATH)/lib -lhdf5 -lz


#
# master to make all versions
#
ALL_TARGETS = clean airfoil_mpi airfoil_cuda airfoil_openmp airfoil_seq airfoil_mpi_genseq airfoil_mpi_cuda airfoil_mpi_cuda_hyb airfoil_mpi_openmp convert_mesh
ifeq ($(OP2_COMPILER),pgi)
	ALL_TARGETS += airfoil_openacc airfoil_mpi_openacc
endif
ifeq ($(OP2_COMPILER),intel)
	ALL_TARGETS += airfoil_mpi_vec
endif

#all: clean airfoil_mpi airfoil_cuda airfoil_openmp airfoil_seq airfoil_mpi_genseq airfoil_mpi_vec airfoil_mpi_cuda airfoil_mpi_openmp airfoil_mpi_cuda_hyb convert_mesh
all: $(ALL_TARGETS)

#
# simple sequential version
#

airfoil_seq: airfoil.cpp save_soln.h adt_calc.h res_calc.h bres_calc.h
	     $(MPICPP) $(CPPFLAGS) airfoil.cpp $(OP2_INC) $(HDF5_INC) $(OP2_LIB) -lop2_seq -lop2_hdf5 $(HDF5_LIB) -o airfoil_seq

#
# x86 version using kernel files generated by op2.py
#

airfoil_openmp:	airfoil_op.cpp airfoil_kernels.cpp \
		save_soln_kernel.cpp  save_soln.h \
		adt_calc_kernel.cpp   adt_calc.h  \
		res_calc_kernel.cpp   res_calc.h  \
		bres_calc_kernel.cpp  bres_calc.h \
		update_kernel.cpp     update.h    \
                Makefile
		$(MPICPP) $(VAR) $(CPPFLAGS) $(OMPFLAGS) $(OP2_INC) $(OP2_LIB) $(HDF5_INC) \
		airfoil_op.cpp -lm airfoil_kernels.cpp -lm -lop2_openmp -lop2_hdf5 $(HDF5_LIB) -o airfoil_openmp

#
# OpenACC version using kernel files generated by op2.py
#

airfoil_openacc: airfoil_op.cpp airfoil_acckernels.c \
                save_soln_acckernel.c  \
                adt_calc_acckernel.c   \
                res_calc_acckernel.c   \
                bres_calc_acckernel.c  \
                update_acckernel.c   \
                Makefile
		$(MPICC) $(VAR) $(CPPFLAGS) $(ACCFLAGS) $(OMPFLAGS) $(OP2_INC) \
                airfoil_acckernels.c -c -o airfoil_acckernels.o
		$(MPICPP) $(VAR) $(CPPFLAGS) $(ACCFLAGS) $(OMPFLAGS) $(OP2_INC) $(OP2_LIB) $(HDF5_INC) \
                airfoil_op.cpp -lm airfoil_acckernels.o -lm $(CUDA_LIB) -lcudart -lop2_cuda -lop2_hdf5 $(HDF5_LIB) -o airfoil_openacc


#
# CUDA version using kernel files generated by op2.py
#

airfoil_cuda:	airfoil_op.cpp airfoil_kernels_cu.o Makefile
		$(MPICPP) $(VAR) $(CPPFLAGS) airfoil_op.cpp airfoil_kernels_cu.o \
		$(CUDA_INC) $(OP2_INC) $(HDF5_INC) \
		$(OP2_LIB) $(CUDA_LIB) -lcudart -lop2_cuda -lop2_hdf5 $(HDF5_LIB) -o airfoil_cuda

airfoil_kernels_cu.o:	airfoil_kernels.cu      \
		save_soln_kernel.cu save_soln.h \
		adt_calc_kernel.cu  adt_calc.h  \
		res_calc_kernel.cu  res_calc.h  \
		bres_calc_kernel.cu bres_calc.h \
		update_kernel.cu    update.h    \
                Makefile
		nvcc  $(VAR) $(INC) $(NVCCFLAGS) $(OP2_INC) $(HDF5_INC) -I /home/gihan/openmpi-intel/include \
		-c -o airfoil_kernels_cu.o airfoil_kernels.cu

#
# mpi with sequential-nodes version
#

airfoil_mpi: airfoil.cpp save_soln.h adt_calc.h res_calc.h bres_calc.h Makefile
	$(MPICPP) $(MPIFLAGS) airfoil.cpp $(OP2_INC) $(PARMETIS_INC) $(PTSCOTCH_INC) $(HDF5_INC) \
	$(OP2_LIB) -lop2_mpi $(PARMETIS_LIB) $(PTSCOTCH_LIB) $(HDF5_LIB) -o airfoil_mpi

#
# mpi genseq version using kernel files generated by op2.py
#

airfoil_mpi_genseq: airfoil_op.cpp airfoil_seqkernels.cpp \
                save_soln_seqkernel.cpp  save_soln.h \
                adt_calc_seqkernel.cpp   adt_calc.h  \
                res_calc_seqkernel.cpp   res_calc.h  \
                bres_calc_seqkernel.cpp  bres_calc.h \
                update_seqkernel.cpp     update.h    \
                Makefile
		$(MPICPP) $(VAR) $(CPPFLAGS) $(OP2_INC) $(OP2_INC) $(HDF5_INC) \
		$(PARMETIS_INC) $(PTSCOTCH_INC) \
		airfoil_op.cpp -lm airfoil_seqkernels.cpp $(OP2_LIB) -lop2_mpi \
		$(PARMETIS_LIB) $(PTSCOTCH_LIB) $(HDF5_LIB) -o airfoil_mpi_genseq

#
# mpi vectorized seq version using kernel files generated by op2.py
#

airfoil_mpi_vec: airfoil_op.cpp airfoil_veckernels.cpp \
                save_soln_veckernel.cpp  save_soln.h \
                adt_calc_veckernel.cpp   adt_calc.h  \
                res_calc_veckernel.cpp   res_calc.h  \
                bres_calc_veckernel.cpp  bres_calc.h \
                update_veckernel.cpp     update.h    \
                Makefile
		$(MPICPP) $(VAR) $(CPPFLAGS) $(OP2_INC) $(OP2_INC) $(HDF5_INC) \
                $(PARMETIS_INC) $(PTSCOTCH_INC) \
                airfoil_op.cpp -lm airfoil_veckernels.cpp $(OP2_LIB) -lop2_mpi \
                $(PARMETIS_LIB) $(PTSCOTCH_LIB) $(HDF5_LIB) -o airfoil_mpi_vec


#
# mpi openmp version using kernel files generated by op2.py
#

airfoil_mpi_openmp: airfoil_op.cpp airfoil_kernels.cpp \
                save_soln_kernel.cpp  save_soln.h \
                adt_calc_kernel.cpp   adt_calc.h  \
                res_calc_kernel.cpp   res_calc.h  \
                bres_calc_kernel.cpp  bres_calc.h \
                update_kernel.cpp     update.h    \
                Makefile
		$(MPICPP) $(VAR) $(CPPFLAGS) $(OMPFLAGS) $(OP2_INC) $(OP2_INC) $(HDF5_INC) \
		$(PARMETIS_INC) $(PTSCOTCH_INC) \
		airfoil_op.cpp -lm airfoil_kernels.cpp $(OP2_LIB) -lop2_mpi \
		$(PARMETIS_LIB) $(PTSCOTCH_LIB) $(HDF5_LIB) -o airfoil_mpi_openmp

#
# mpi openacc version using kernel files generated by op2.py
#

airfoil_mpi_openacc: airfoil_op.cpp airfoil_acckernels.c \
                save_soln_acckernel.c  save_soln.h \
                adt_calc_acckernel.c   adt_calc.h  \
                res_calc_acckernel.c   res_calc.h  \
                bres_calc_acckernel.c  bres_calc.h \
                update_acckernel.c     update.h    \
                Makefile
		$(MPICC) $(VAR) $(CPPFLAGS) $(ACCFLAGS) $(OMPFLAGS) $(OP2_INC) \
		airfoil_acckernels.c -c -o airfoil_acckernels.o
		$(MPICPP) $(VAR) $(CPPFLAGS) $(ACCFLAGS) $(OP2_INC) $(OP2_INC) $(HDF5_INC) \
		$(PARMETIS_INC) $(PTSCOTCH_INC) \
		airfoil_op.cpp -lm airfoil_acckernels.o -DOPENACC $(CUDA_LIB) -lcudart $(OP2_LIB) -lop2_mpi_cuda \
		$(PARMETIS_LIB) $(PTSCOTCH_LIB) $(HDF5_LIB) -o airfoil_mpi_openacc
#
# mpi with CUDA version
#

airfoil_mpi_cuda: airfoil_op.cpp airfoil_kernels_mpi_cu.o Makefile
		  $(MPICPP) $(MPIFLAGS) airfoil_op.cpp -lm airfoil_kernels_mpi_cu.o \
		  $(OP2_INC) $(PARMETIS_INC) $(PTSCOTCH_INC) $(HDF5_INC) \
		  $(OP2_LIB) -lop2_mpi_cuda $(PARMETIS_LIB) $(PTSCOTCH_LIB) \
		  $(HDF5_LIB) $(CUDA_LIB) -lcudart -o airfoil_mpi_cuda

airfoil_kernels_mpi_cu.o: airfoil_kernels.cu \
		save_soln_kernel.cu  save_soln.h \
		adt_calc_kernel.cu   adt_calc.h  \
		res_calc_kernel.cu   res_calc.h  \
		bres_calc_kernel.cu  bres_calc.h \
		update_kernel.cu     update.h    \
		Makefile
		nvcc  $(INC) $(NVCCFLAGS) $(OP2_INC) -I $(MPI_INSTALL_PATH)/include \
		-c -o airfoil_kernels_mpi_cu.o airfoil_kernels.cu

#
# Hybrid CPU+GPU version
#

airfoil_mpi_cuda_hyb: airfoil_op.cpp airfoil_hybkernels.o Makefile
		  $(MPICPP) $(MPIFLAGS) -DOP_HYBRID_GPU airfoil_op.cpp -lm airfoil_hybkernels.o airfoil_hybkernels2.o \
		  $(OP2_INC) $(PARMETIS_INC) $(PTSCOTCH_INC) $(HDF5_INC) \
		  $(OP2_LIB) -lop2_mpi_cuda $(PARMETIS_LIB) $(PTSCOTCH_LIB) \
		  $(HDF5_LIB) $(CUDA_LIB) -lcudart -o airfoil_mpi_cuda_hyb

airfoil_hybkernels.o: airfoil_hybkernels.cu \
                airfoil_kernels.cu airfoil_kernels.cpp save_soln.h \
                adt_calc.h  res_calc.h  \
                bres_calc.h update.h    \
                Makefile
		nvcc -DOP_HYBRID_GPU -DGPUPASS $(INC) $(NVCCFLAGS) $(OP2_INC) -I $(MPI_INSTALL_PATH)/include \
                -c -o airfoil_hybkernels.o airfoil_hybkernels.cu
		nvcc -DOP_HYBRID_GPU -Xcompiler="$(OMPFLAGS)" $(INC) $(NVCCFLAGS) $(OP2_INC) -I $(MPI_INSTALL_PATH)/include \
                -c -o airfoil_hybkernels2.o airfoil_hybkernels.cu
#
# convert ASCI new_gird.dat to HDF5 new_grid.h5
#

convert_mesh: convert_mesh.cpp
	$(MPICPP) $(MPIFLAGS) convert_mesh.cpp $(OP2_INC) $(PARMETIS_INC) $(PTSCOTCH_INC) $(HDF5_INC) \
	$(OP2_LIB) -lop2_mpi $(PARMETIS_LIB) $(PTSCOTCH_LIB) $(HDF5_LIB) -o convert_mesh




#
# cleanup
#

clean:
		rm -f airfoil_seq airfoil_openmp airfoil_cuda airfoil_mpi airfoil_mpi_genseq airfoil_mpi_vec airfoil_mpi_cuda_hyb airfoil_mpi_openmp airfoil_mpi_cuda convert_mesh airfoil_openacc airfoil_mpi_openacc *.o *.optrpt
