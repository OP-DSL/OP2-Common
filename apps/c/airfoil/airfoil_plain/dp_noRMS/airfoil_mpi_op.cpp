//
// auto-generated by op2.py
//

/*
 * Open source copyright declaration based on BSD open source template:
 * http://www.opensource.org/licenses/bsd-license.php
 *
 * This file is part of the OP2 distribution.
 *
 * Copyright (c) 2011, Mike Giles and others. Please see the AUTHORS file in
 * the main source directory for a full list of copyright holders.
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * The name of Mike Giles may not be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY Mike Giles ''AS IS'' AND ANY
 * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL Mike Giles BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

//
//     Nonlinear airfoil lift calculation
//
//     Written by Mike Giles, 2010-2011, based on FORTRAN code
//     by Devendra Ghate and Mike Giles, 2005
//
//     Extended to MPI by Gihan Mudalige March 2011

//
// standard headers
//

#include <math.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <algorithm>

//
// mpi header file - included by user for user level mpi
//

#include <mpi.h>

// global constants

double gam, gm1, cfl, eps, mach, alpha, qinf[4];

//
// OP header file
//

#include "op_lib_mpi.h"
#include  "op_lib_cpp.h"
#include <mpi.h>

//#include <op_lib_core.h>
#include <op_lib_c.h>
#include <op_util.h>

#include <op_mpi_core.h>

#ifdef SLOPE
#include "executor.h"
#include "inspector.h"
#define TILE_SIZE 5000
#endif

//
// op_par_loop declarations
//
#ifdef OPENACC
#ifdef __cplusplus
extern "C" {
#endif
#endif

void op_par_loop_save_soln(char const *, op_set,
  op_arg,
  op_arg );

void op_par_loop_adt_calc(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_res_calc(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_bres_calc(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_update(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg );

void op_par_loop_update1(char const *, op_set,
  op_arg,
  op_arg,
  op_arg,
  op_arg,
  op_arg );
#ifdef OPENACC
#ifdef __cplusplus
}
#endif
#endif


//
// kernel routines for parallel loops
//

#include "adt_calc.h"
#include "bres_calc.h"
#include "res_calc.h"
#include "save_soln.h"
#include "update.h"
#include "update1.h"

//
// user declared functions
//

static int compute_local_size(int global_size, int mpi_comm_size,
                              int mpi_rank) {
  int local_size = global_size / mpi_comm_size;
  int remainder = (int)fmod(global_size, mpi_comm_size);

  if (mpi_rank < remainder) {
    local_size = local_size + 1;
  }
  return local_size;
}

static void scatter_double_array(double *g_array, double *l_array,
                                 int comm_size, int g_size, int l_size,
                                 int elem_size) {
  int *sendcnts = (int *)malloc(comm_size * sizeof(int));
  int *displs = (int *)malloc(comm_size * sizeof(int));
  int disp = 0;

  for (int i = 0; i < comm_size; i++) {
    sendcnts[i] = elem_size * compute_local_size(g_size, comm_size, i);
  }
  for (int i = 0; i < comm_size; i++) {
    displs[i] = disp;
    disp = disp + sendcnts[i];
  }

  MPI_Scatterv(g_array, sendcnts, displs, MPI_DOUBLE, l_array,
               l_size * elem_size, MPI_DOUBLE, MPI_ROOT, MPI_COMM_WORLD);

  free(sendcnts);
  free(displs);
}

static void scatter_int_array(int *g_array, int *l_array, int comm_size,
                              int g_size, int l_size, int elem_size) {
  int *sendcnts = (int *)malloc(comm_size * sizeof(int));
  int *displs = (int *)malloc(comm_size * sizeof(int));
  int disp = 0;

  for (int i = 0; i < comm_size; i++) {
    sendcnts[i] = elem_size * compute_local_size(g_size, comm_size, i);
  }
  for (int i = 0; i < comm_size; i++) {
    displs[i] = disp;
    disp = disp + sendcnts[i];
  }

  MPI_Scatterv(g_array, sendcnts, displs, MPI_INT, l_array, l_size * elem_size,
               MPI_INT, MPI_ROOT, MPI_COMM_WORLD);

  free(sendcnts);
  free(displs);
}

static void check_scan(int items_received, int items_expected) {
  if (items_received != items_expected) {
    op_printf("error reading from new_grid.dat\n");
    exit(-1);
  }
}


#ifdef SLOPE

int get_max_value(int* arr, int size){
  int max = 0;  // assumption: max >= 0
  for(int i = 0; i < size; i++){
    if(max < arr[i]){
      max = arr[i];
    }  
  }
  return max;
}

void calculate_max_values(op_set* sets, int set_count, op_map* maps, int map_count,
std::map<op_set, int>* to_set_to_core_max, std::map<op_set, int>* to_set_to_exec_max, std::map<op_set, int>* to_set_to_nonexec_max){

  std::map<op_set, std::vector<int>> to_set_to_map_index;
  std::map<op_set, std::vector<int>>::iterator it;
  op_set to_set;

  for(int i = 0; i < map_count; i ++){
    to_set = maps[i]->to;
    it = to_set_to_map_index.find(to_set);
    if(it != to_set_to_map_index.end()){
      std::vector<int>* map_ids = &it->second;
      map_ids->push_back(i);
    }
    else{
      std::vector<int> map_ids;
      map_ids.push_back(i);
      to_set_to_map_index.insert(std::pair<op_set, std::vector<int>>(to_set, map_ids));
    }
  }
  
  for (auto it = to_set_to_map_index.begin(); it != to_set_to_map_index.end(); ++it){
    //get core max values
    int core_max[it->second.size()];
    for(int i = 0; i < it->second.size(); i++){
      core_max[i] = get_max_value(maps[it->second.at(i)]->map, maps[it->second.at(i)]->from->core_size * maps[it->second.at(i)]->dim);
    }

    int core_max_of_max = core_max[0];
    for(int i = 0; i < it->second.size(); i++){
      if(core_max_of_max < core_max[i])
        core_max_of_max = core_max[i];
    }
    to_set_to_core_max->insert(std::pair<op_set, int>(it->first, core_max_of_max));

    //get exec max values
    int exec_max[it->second.size()];
    for(int i = 0; i < it->second.size(); i++){
      exec_max[i] = get_max_value(maps[it->second.at(i)]->map, 
      (maps[it->second.at(i)]->from->size +  OP_import_exec_list[maps[it->second.at(i)]->from->index]->size) * maps[it->second.at(i)]->dim);
    }

    int exec_max_of_max = exec_max[0];
    for(int i = 0; i < it->second.size(); i++){
      if(exec_max_of_max < exec_max[i])
        exec_max_of_max = exec_max[i];
    }
    to_set_to_exec_max->insert(std::pair<op_set, int>(it->first, exec_max_of_max));

    //get nonexec max values
    int nonexec_max[it->second.size()];
    for(int i = 0; i < it->second.size(); i++){
      nonexec_max[i] = get_max_value(maps[it->second.at(i)]->map, 
      (maps[it->second.at(i)]->from->size +  OP_import_exec_list[maps[it->second.at(i)]->from->index]->size + 
      OP_import_nonexec_list[maps[it->second.at(i)]->from->index]->size) * maps[it->second.at(i)]->dim);
    }

    int nonexec_max_of_max = nonexec_max[0];
    for(int i = 0; i < it->second.size(); i++){
      if(nonexec_max_of_max < nonexec_max[i])
        nonexec_max_of_max = nonexec_max[i];
    }
    to_set_to_nonexec_max->insert(std::pair<op_set, int>(it->first, nonexec_max_of_max));
  } 
}

int get_core_size(op_set set, std::map<op_set, int>* to_set_to_core_max){

  std::map<op_set, int>::iterator it;
  it = to_set_to_core_max->find(set);

  if(it != to_set_to_core_max->end()){
    return it->second + 1;
  }
  else{
    return set->core_size;
  }
}

int get_exec_size(op_set set, std::map<op_set, int>* to_set_to_core_max, std::map<op_set, int>* to_set_to_exec_max){

  std::map<op_set, int>::iterator it_core, it_exec;
  it_core = to_set_to_core_max->find(set);

  if(it_core != to_set_to_core_max->end()){
    it_exec = to_set_to_exec_max->find(set);
    return ((it_exec->second - it_core->second) > 0) ? (it_exec->second - it_core->second) : 0;
  }
  else{
    return (set->size - set->core_size) + OP_import_exec_list[set->index]->size;
  }
}

int get_nonexec_size(op_set set, std::map<op_set, int>* to_set_to_exec_max, std::map<op_set, int>* to_set_to_nonexec_max){

  std::map<op_set, int>::iterator it_exec, it_nonexec;
  it_exec = to_set_to_exec_max->find(set);

  if(it_exec != to_set_to_exec_max->end()){
    it_nonexec = to_set_to_nonexec_max->find(set);
    return ((it_nonexec->second - it_exec->second) > 0) ? (it_nonexec->second - it_exec->second) : 0;
  }
  else{
    return OP_import_nonexec_list[set->index]->size;
  }
}
#endif

//
// main program
//

int main(int argc, char **argv) {
  // OP initialisation
  op_init(argc, argv, 2);

  // MPI for user I/O
  int my_rank;
  int comm_size;
  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);

  // timer
  double cpu_t1, cpu_t2, wall_t1, wall_t2;

  int *becell, *ecell, *bound, *bedge, *edge, *cell;
  double *x, *q, *qold, *adt, *res;

  int nnode, ncell, nedge, nbedge, niter;
  double rms;

  /**------------------------BEGIN I/O and PARTITIONING -------------------**/

  op_timers(&cpu_t1, &wall_t1);

  /* read in grid from disk on root processor */
  FILE *fp;

  if ((fp = fopen("new_grid.dat", "r")) == NULL) {
    op_printf("can't open file new_grid.dat\n");
    exit(-1);
  }

  int g_nnode, g_ncell, g_nedge, g_nbedge;

  check_scan(
      fscanf(fp, "%d %d %d %d \n", &g_nnode, &g_ncell, &g_nedge, &g_nbedge), 4);

  int *g_becell = 0, *g_ecell = 0, *g_bound = 0, *g_bedge = 0, *g_edge = 0,
      *g_cell = 0;
  double *g_x = 0, *g_q = 0, *g_qold = 0, *g_adt = 0, *g_res = 0;

  // set constants

  op_printf("initialising flow field\n");
  gam = 1.4f;
  gm1 = gam - 1.0f;
  cfl = 0.9f;
  eps = 0.05f;

  double mach = 0.4f;
  double alpha = 3.0f * atan(1.0f) / 45.0f;
  double p = 1.0f;
  double r = 1.0f;
  double u = sqrt(gam * p / r) * mach;
  double e = p / (r * gm1) + 0.5f * u * u;

  qinf[0] = r;
  qinf[1] = r * u;
  qinf[2] = 0.0f;
  qinf[3] = r * e;

  op_printf("reading in grid \n");
  op_printf("Global number of nodes, cells, edges, bedges = %d, %d, %d, %d\n",
            g_nnode, g_ncell, g_nedge, g_nbedge);

  if (my_rank == MPI_ROOT) {
    g_cell = (int *)malloc(4 * g_ncell * sizeof(int));
    g_edge = (int *)malloc(2 * g_nedge * sizeof(int));
    g_ecell = (int *)malloc(2 * g_nedge * sizeof(int));
    g_bedge = (int *)malloc(2 * g_nbedge * sizeof(int));
    g_becell = (int *)malloc(g_nbedge * sizeof(int));
    g_bound = (int *)malloc(g_nbedge * sizeof(int));

    g_x = (double *)malloc(2 * g_nnode * sizeof(double));
    g_q = (double *)malloc(4 * g_ncell * sizeof(double));
    g_qold = (double *)malloc(4 * g_ncell * sizeof(double));
    g_res = (double *)malloc(4 * g_ncell * sizeof(double));
    g_adt = (double *)malloc(g_ncell * sizeof(double));

    for (int n = 0; n < g_nnode; n++) {
      check_scan(fscanf(fp, "%lf %lf \n", &g_x[2 * n], &g_x[2 * n + 1]), 2);
    }

    for (int n = 0; n < g_ncell; n++) {
      check_scan(fscanf(fp, "%d %d %d %d \n", &g_cell[4 * n],
                        &g_cell[4 * n + 1], &g_cell[4 * n + 2],
                        &g_cell[4 * n + 3]),
                 4);
    }

    for (int n = 0; n < g_nedge; n++) {
      check_scan(fscanf(fp, "%d %d %d %d \n", &g_edge[2 * n],
                        &g_edge[2 * n + 1], &g_ecell[2 * n],
                        &g_ecell[2 * n + 1]),
                 4);
    }

    for (int n = 0; n < g_nbedge; n++) {
      check_scan(fscanf(fp, "%d %d %d %d \n", &g_bedge[2 * n],
                        &g_bedge[2 * n + 1], &g_becell[n], &g_bound[n]),
                 4);
    }

    // initialise flow field and residual

    for (int n = 0; n < g_ncell; n++) {
      for (int m = 0; m < 4; m++) {
        g_q[4 * n + m] = qinf[m];
        g_res[4 * n + m] = 0.0f;
      }
    }
  }

  fclose(fp);

  nnode = compute_local_size(g_nnode, comm_size, my_rank);
  ncell = compute_local_size(g_ncell, comm_size, my_rank);
  nedge = compute_local_size(g_nedge, comm_size, my_rank);
  nbedge = compute_local_size(g_nbedge, comm_size, my_rank);

  op_printf(
      "Number of nodes, cells, edges, bedges on process %d = %d, %d, %d, %d\n",
      my_rank, nnode, ncell, nedge, nbedge);

  /*Allocate memory to hold local sets, mapping tables and data*/
  cell = (int *)malloc(4 * ncell * sizeof(int));
  edge = (int *)malloc(2 * nedge * sizeof(int));
  ecell = (int *)malloc(2 * nedge * sizeof(int));
  bedge = (int *)malloc(2 * nbedge * sizeof(int));
  becell = (int *)malloc(nbedge * sizeof(int));
  bound = (int *)malloc(nbedge * sizeof(int));

  x = (double *)malloc(2 * nnode * sizeof(double));
  q = (double *)malloc(4 * ncell * sizeof(double));
  qold = (double *)malloc(4 * ncell * sizeof(double));
  res = (double *)malloc(4 * ncell * sizeof(double));
  adt = (double *)malloc(ncell * sizeof(double));

  /* scatter sets, mappings and data on sets*/
  scatter_int_array(g_cell, cell, comm_size, g_ncell, ncell, 4);
  scatter_int_array(g_edge, edge, comm_size, g_nedge, nedge, 2);
  scatter_int_array(g_ecell, ecell, comm_size, g_nedge, nedge, 2);
  scatter_int_array(g_bedge, bedge, comm_size, g_nbedge, nbedge, 2);
  scatter_int_array(g_becell, becell, comm_size, g_nbedge, nbedge, 1);
  scatter_int_array(g_bound, bound, comm_size, g_nbedge, nbedge, 1);

  scatter_double_array(g_x, x, comm_size, g_nnode, nnode, 2);
  scatter_double_array(g_q, q, comm_size, g_ncell, ncell, 4);
  scatter_double_array(g_qold, qold, comm_size, g_ncell, ncell, 4);
  scatter_double_array(g_res, res, comm_size, g_ncell, ncell, 4);
  scatter_double_array(g_adt, adt, comm_size, g_ncell, ncell, 1);

  /*Freeing memory allocated to gloabal arrays on rank 0
    after scattering to all processes*/
  if (my_rank == MPI_ROOT) {
    free(g_cell);
    free(g_edge);
    free(g_ecell);
    free(g_bedge);
    free(g_becell);
    free(g_bound);
    free(g_x);
    free(g_q);
    free(g_qold);
    free(g_adt);
    free(g_res);
  }

  op_timers(&cpu_t2, &wall_t2);
  op_printf("Max total file read time = %f\n", wall_t2 - wall_t1);

  /**------------------------END I/O and PARTITIONING -----------------------**/

  // declare sets, pointers, datasets and global constants

  op_set nodes = op_decl_set(nnode, "nodes");
  op_set edges = op_decl_set(nedge, "edges");
  op_set bedges = op_decl_set(nbedge, "bedges");
  op_set cells = op_decl_set(ncell, "cells");

  op_map pedge = op_decl_map(edges, nodes, 2, edge, "pedge");
  op_map pecell = op_decl_map(edges, cells, 2, ecell, "pecell");
  op_map pbedge = op_decl_map(bedges, nodes, 2, bedge, "pbedge");
  op_map pbecell = op_decl_map(bedges, cells, 1, becell, "pbecell");
  op_map pcell = op_decl_map(cells, nodes, 4, cell, "pcell");

  op_dat p_bound = op_decl_dat(bedges, 1, "int", bound, "p_bound");
  op_dat p_x = op_decl_dat(nodes, 2, "double", x, "p_x");
  op_dat p_q = op_decl_dat(cells, 4, "double", q, "p_q");
  op_dat p_qold = op_decl_dat(cells, 4, "double", qold, "p_qold");
  op_dat p_adt = op_decl_dat(cells, 1, "double", adt, "p_adt");
  op_dat p_res = op_decl_dat(cells, 4, "double", res, "p_res");

  op_decl_const2("gam",1,"double",&gam);
  op_decl_const2("gm1",1,"double",&gm1);
  op_decl_const2("cfl",1,"double",&cfl);
  op_decl_const2("eps",1,"double",&eps);
  op_decl_const2("mach",1,"double",&mach);
  op_decl_const2("alpha",1,"double",&alpha);
  op_decl_const2("qinf",4,"double",qinf);

  op_diagnostic_output();

  // trigger partitioning and halo creation routines
  op_partition("DONOT", "KWAY", cells, pecell, p_x);  // slope:avoid doing an mpi partitioning

  // initialise timers for total execution wall time

  #ifdef SLOPE

  int set_size = 0;
  op_arg args0[6];
  args0[0] = op_arg_dat(p_x,0,pcell,2,"double",OP_READ);
  args0[1] = op_arg_dat(p_x,1,pcell,2,"double",OP_READ);
  args0[2] = op_arg_dat(p_x,2,pcell,2,"double",OP_READ);
  args0[3] = op_arg_dat(p_x,3,pcell,2,"double",OP_READ);
  args0[4] = op_arg_dat(p_q,-1,OP_ID,4,"double",OP_READ);
  args0[5] = op_arg_dat(p_adt,-1,OP_ID,1,"double",OP_WRITE);

  op_arg args1[8];
  args1[0] = op_arg_dat(p_x,0,pedge,2,"double",OP_READ);
  args1[1] = op_arg_dat(p_x,1,pedge,2,"double",OP_READ);
  args1[2] = op_arg_dat(p_q,0,pecell,4,"double",OP_READ);
  args1[3] = op_arg_dat(p_q,1,pecell,4,"double",OP_READ);
  args1[4] = op_arg_dat(p_adt,0,pecell,1,"double",OP_READ);
  args1[5] = op_arg_dat(p_adt,1,pecell,1,"double",OP_READ);
  args1[6] = op_arg_dat(p_res,0,pecell,4,"double",OP_INC);
  args1[7] = op_arg_dat(p_res,1,pecell,4,"double",OP_INC);
  
  op_arg args2[6];
  args2[0] = op_arg_dat(p_x,0,pbedge,2,"double",OP_READ);
  args2[1] = op_arg_dat(p_x,1,pbedge,2,"double",OP_READ);
  args2[2] = op_arg_dat(p_q,0,pbecell,4,"double",OP_READ);
  args2[3] = op_arg_dat(p_adt,0,pbecell,1,"double",OP_READ);
  args2[4] = op_arg_dat(p_res,0,pbecell,4,"double",OP_INC);
  args2[5] = op_arg_dat(p_bound,-1,OP_ID,1,"int",OP_READ);

  op_arg args3[4];
  args3[0] = op_arg_dat(p_qold,-1,OP_ID,4,"double",OP_READ);
  args3[1] = op_arg_dat(p_q,-1,OP_ID,4,"double",OP_WRITE);
  args3[2] = op_arg_dat(p_res,-1,OP_ID,4,"double",OP_RW);
  args3[3] = op_arg_dat(p_adt,-1,OP_ID,1,"double",OP_READ);


  int avgTileSize = 5000;
  if(argc > 1){
    avgTileSize = atoi(argv[1]);
  }
  
  int seedTilePoint = 0;

  op_set* sets = new op_set[4];
  sets[0] = nodes;
  sets[1] = edges;
  sets[2] = bedges;
  sets[3] = cells;

  op_map* maps = new op_map[5];
  maps[0] = pedge;
  maps[1] = pecell;
  maps[2] = pbedge;
  maps[3] = pbecell;
  maps[4] = pcell;

  std::map<op_set, int> to_set_to_core_max;
  std::map<op_set, int> to_set_to_exec_max;
  std::map<op_set, int> to_set_to_nonexec_max;

  calculate_max_values(sets, 4, maps, 5, &to_set_to_core_max, &to_set_to_exec_max, &to_set_to_nonexec_max);

  //sets
  set_t* sl_nodes = set("nodes", get_core_size(nodes, &to_set_to_core_max) , get_exec_size(nodes, &to_set_to_core_max, &to_set_to_exec_max), 
    get_nonexec_size(nodes, &to_set_to_exec_max, &to_set_to_nonexec_max));
  set_t* sl_edges = set("edges", get_core_size(edges, &to_set_to_core_max) , get_exec_size(edges, &to_set_to_core_max, &to_set_to_exec_max), 
    get_nonexec_size(edges, &to_set_to_exec_max, &to_set_to_nonexec_max));
  set_t* sl_bedges = set("bedges", get_core_size(bedges, &to_set_to_core_max) , get_exec_size(bedges, &to_set_to_core_max, &to_set_to_exec_max), 
    get_nonexec_size(bedges, &to_set_to_exec_max, &to_set_to_nonexec_max));
  set_t* sl_cells = set("cells", get_core_size(cells, &to_set_to_core_max), get_exec_size(cells, &to_set_to_core_max, &to_set_to_exec_max), 
    get_nonexec_size(cells, &to_set_to_exec_max, &to_set_to_nonexec_max));

  //maps
  map_t* sl_pcell = map("c2n", sl_cells, sl_nodes, pcell->map, sl_cells->size * pcell->dim);
  map_t* sl_pedge = map("e2n", sl_edges, sl_nodes, pedge->map, sl_edges->size * pedge->dim);
  map_t* sl_pecell = map("e2c", sl_edges, sl_cells, pecell->map, sl_edges->size * pecell->dim);
  map_t* sl_pbedge = map("be2n", sl_bedges, sl_nodes, pbedge->map, sl_bedges->size * pbedge->dim);
  map_t* sl_pbecell = map("be2c", sl_bedges, sl_cells, pbecell->map, sl_bedges->size * pbecell->dim);

  // descriptors
  desc_list adtCalcDesc ({desc(sl_pcell, READ),
                          desc(DIRECT, WRITE)});
  desc_list resCalcDesc ({desc(sl_pedge, READ),
                          desc(sl_pecell, READ),
                          desc(sl_pecell, INC)});
  desc_list bresCalcDesc ({desc(sl_pbedge, READ),
                           desc(sl_pbecell, READ),
                           desc(sl_pbecell, INC)});
  desc_list updateDesc ({desc(DIRECT, READ),
                         desc(DIRECT, WRITE)});
  
  // inspector_t* insp = insp_init(avgTileSize, ONLY_MPI);
  inspector_t* insp = insp_init(avgTileSize, OMP_MPI);

  insp_add_parloop (insp, "adtCalc", sl_cells, &adtCalcDesc);
  insp_add_parloop (insp, "resCalc", sl_edges, &resCalcDesc);
  insp_add_parloop (insp, "bresCalc", sl_bedges, &bresCalcDesc);
  insp_add_parloop (insp, "update", sl_cells, &updateDesc);
 
  insp_run (insp, seedTilePoint);

  for (int i = 0; i < comm_size; i++) {
    if (i == my_rank) {
      insp_print (insp, LOW);
      // generate_vtk (insp, HIGH, vertices, mesh->coords, DIM2, rank);
    }
    MPI_Barrier(MPI_COMM_WORLD);
  }

  printf("running executor\n");
  executor_t* exec = exec_init (insp);
  int nColors = exec_num_colors (exec);

  #endif

  op_timers(&cpu_t1, &wall_t1);

  niter = 1000;
  for (int iter = 1; iter <= niter; iter++) {

    // save old flow solution
    op_par_loop_save_soln("save_soln",cells,
                op_arg_dat(p_q,-1,OP_ID,4,"double",OP_READ),
                op_arg_dat(p_qold,-1,OP_ID,4,"double",OP_WRITE));

    //  predictor/corrector update loop
    #ifdef SLOPE
    for (int m = 0; m < 2; m++) {
      for (int i = 0; i < nColors; i++) {

        // for all tiles of this color
        const int nTilesPerColor = exec_tiles_per_color (exec, i);

        #pragma omp parallel for
        for (int j = 0; j < nTilesPerColor; j++) {

          // execute the tile
          tile_t* tile = exec_tile_at (exec, i, j, LOCAL);
          if(!tile)
            continue;
         
          int tileLoopSize;

          // loop adt_calc (calculate area/timstep)
          iterations_list& lc2n_0 = tile_get_local_map (tile, 0, "c2n");
          iterations_list& iterations_0 = tile_get_iterations (tile, 0);
          tileLoopSize = tile_loop_size (tile, 0);

          #pragma omp simd
          for (int k = 0; k < tileLoopSize; k++) {
            adt_calc (((double*)(p_x->data)) + lc2n_0[k*4 + 0]*2,
                      ((double*)(p_x->data)) + lc2n_0[k*4 + 1]*2,
                      ((double*)(p_x->data)) + lc2n_0[k*4 + 2]*2,
                      ((double*)(p_x->data)) + lc2n_0[k*4 + 3]*2,
                      ((double*)(p_q->data)) + iterations_0[k]*4,
                      ((double*)(p_adt->data)) + iterations_0[k]);
          }

          // loop res_calc
          iterations_list& le2n_1 = tile_get_local_map (tile, 1, "e2n");
          iterations_list& le2c_1 = tile_get_local_map (tile, 1, "e2c");
          iterations_list& iterations_1 = tile_get_iterations (tile, 1);
          tileLoopSize = tile_loop_size (tile, 1);

          for (int k = 0; k < tileLoopSize; k++) {
            res_calc (((double*)(p_x->data)) + le2n_1[k*2 + 0]*2,
                      ((double*)(p_x->data)) + le2n_1[k*2 + 1]*2,
                      ((double*)(p_q->data)) + le2c_1[k*2 + 0]*4,
                      ((double*)(p_q->data)) + le2c_1[k*2 + 1]*4,
                      ((double*)(p_adt->data)) + le2c_1[k*2 + 0]*1,
                      ((double*)(p_adt->data)) + le2c_1[k*2 + 1]*1,
                      ((double*)(p_res->data)) + le2c_1[k*2 + 0]*4,
                      ((double*)(p_res->data)) + le2c_1[k*2 + 1]*4);
          }

          // loop bres_calc
          iterations_list& lbe2n_2 = tile_get_local_map (tile, 2, "be2n");
          iterations_list& lbe2c_2 = tile_get_local_map (tile, 2, "be2c");
          iterations_list& iterations_2 = tile_get_iterations (tile, 2);
          tileLoopSize = tile_loop_size (tile, 2);

          for (int k = 0; k < tileLoopSize; k++) {
            bres_calc (((double*)(p_x->data)) + lbe2n_2[k*2 + 0]*2,
                       ((double*)(p_x->data)) + lbe2n_2[k*2 + 1]*2,
                       ((double*)(p_q->data)) + lbe2c_2[k + 0]*4,
                       ((double*)(p_adt->data)) + lbe2c_2[k + 0]*1,
                       ((double*)(p_res->data)) + lbe2c_2[k + 0]*4,
                       ((int*)(p_bound->data)) + iterations_2[k]);
          }

          // loop update
          iterations_list& iterations_3 = tile_get_iterations (tile, 3);
          tileLoopSize = tile_loop_size (tile, 3);

          for (int k = 0; k < tileLoopSize; k++) {
            update    (((double*)(p_qold->data)) + iterations_3[k]*4,
                       ((double*)(p_q->data)) + iterations_3[k]*4,
                       ((double*)(p_res->data)) + iterations_3[k]*4,
                       ((double*)(p_adt->data)) + iterations_3[k]);

          }
        }
      }
    
      op_mpi_set_dirtybit(6, args0);
      op_mpi_set_dirtybit(8, args1);
      op_mpi_set_dirtybit(6, args2);
      op_mpi_set_dirtybit(4, args3);
 
      set_size = op_mpi_halo_exchanges(cells, 6, args0);
      set_size = op_mpi_halo_exchanges(edges, 8, args1);
      set_size = op_mpi_halo_exchanges(bedges, 6, args2);
      set_size = op_mpi_halo_exchanges(cells, 4, args3);

      op_mpi_wait_all(6, args0);
      op_mpi_wait_all(8, args1);
      op_mpi_wait_all(6, args2);
      op_mpi_wait_all(4, args3);


      for (int i = 0; i < nColors; i++) {

        // for all tiles of this color
        const int nTilesPerColor = exec_tiles_per_color (exec, i);
        #pragma omp parallel for
        for (int j = 0; j < nTilesPerColor; j++) {

          // execute the tile
          tile_t* tile = exec_tile_at (exec, i, j, EXEC_HALO);
          if(tile == NULL)
            continue;
      
          int tileLoopSize;

          // loop adt_calc (calculate area/timstep)
          iterations_list& lc2n_0 = tile_get_local_map (tile, 0, "c2n");
          iterations_list& iterations_0 = tile_get_iterations (tile, 0);
          tileLoopSize = tile_loop_size (tile, 0);

          #pragma omp simd
          for (int k = 0; k < tileLoopSize; k++) {
            adt_calc (((double*)(p_x->data)) + lc2n_0[k*4 + 0]*2,
                      ((double*)(p_x->data)) + lc2n_0[k*4 + 1]*2,
                      ((double*)(p_x->data)) + lc2n_0[k*4 + 2]*2,
                      ((double*)(p_x->data)) + lc2n_0[k*4 + 3]*2,
                      ((double*)(p_q->data)) + iterations_0[k]*4,
                      ((double*)(p_adt->data)) + iterations_0[k]);
          }
            
          // loop res_calc
          iterations_list& le2n_1 = tile_get_local_map (tile, 1, "e2n");
          iterations_list& le2c_1 = tile_get_local_map (tile, 1, "e2c");
          iterations_list& iterations_1 = tile_get_iterations (tile, 1);
          tileLoopSize = tile_loop_size (tile, 1);

          for (int k = 0; k < tileLoopSize; k++) {
            res_calc (((double*)(p_x->data)) + le2n_1[k*2 + 0]*2,
                      ((double*)(p_x->data)) + le2n_1[k*2 + 1]*2,
                      ((double*)(p_q->data)) + le2c_1[k*2 + 0]*4,
                      ((double*)(p_q->data)) + le2c_1[k*2 + 1]*4,
                      ((double*)(p_adt->data)) + le2c_1[k*2 + 0]*1,
                      ((double*)(p_adt->data)) + le2c_1[k*2 + 1]*1,
                      ((double*)(p_res->data)) + le2c_1[k*2 + 0]*4,
                      ((double*)(p_res->data)) + le2c_1[k*2 + 1]*4);
          }

          // loop bres_calc
          iterations_list& lbe2n_2 = tile_get_local_map (tile, 2, "be2n");
          iterations_list& lbe2c_2 = tile_get_local_map (tile, 2, "be2c");
          iterations_list& iterations_2 = tile_get_iterations (tile, 2);
          tileLoopSize = tile_loop_size (tile, 2);

          for (int k = 0; k < tileLoopSize; k++) {
            bres_calc (((double*)(p_x->data)) + lbe2n_2[k*2 + 0]*2,
                       ((double*)(p_x->data)) + lbe2n_2[k*2 + 1]*2,
                       ((double*)(p_q->data)) + lbe2c_2[k + 0]*4,
                       ((double*)(p_adt->data)) + lbe2c_2[k + 0]*1,
                       ((double*)(p_res->data)) + lbe2c_2[k + 0]*4,
                       ((int*)(p_bound->data)) + iterations_2[k]);
          }

          // loop update
          iterations_list& iterations_3 = tile_get_iterations (tile, 3);
          tileLoopSize = tile_loop_size (tile, 3);

          for (int k = 0; k < tileLoopSize; k++) {
            update    (((double*)(p_qold->data)) + iterations_3[k]*4,
                       ((double*)(p_q->data)) + iterations_3[k]*4,
                       ((double*)(p_res->data)) + iterations_3[k]*4,
                       ((double*)(p_adt->data)) + iterations_3[k]);
          }
        }
      }
    }
    #else
    for (int m = 0; m < 2; m++) {
      //    calculate area/timstep
      op_par_loop_adt_calc("adt_calc",cells,
                  op_arg_dat(p_x,0,pcell,2,"double",OP_READ),
                  op_arg_dat(p_x,1,pcell,2,"double",OP_READ),
                  op_arg_dat(p_x,2,pcell,2,"double",OP_READ),
                  op_arg_dat(p_x,3,pcell,2,"double",OP_READ),
                  op_arg_dat(p_q,-1,OP_ID,4,"double",OP_READ),
                  op_arg_dat(p_adt,-1,OP_ID,1,"double",OP_WRITE));

      //    calculate flux residual
      op_par_loop_res_calc("res_calc",edges,
                  op_arg_dat(p_x,0,pedge,2,"double",OP_READ),
                  op_arg_dat(p_x,1,pedge,2,"double",OP_READ),
                  op_arg_dat(p_q,0,pecell,4,"double",OP_READ),
                  op_arg_dat(p_q,1,pecell,4,"double",OP_READ),
                  op_arg_dat(p_adt,0,pecell,1,"double",OP_READ),
                  op_arg_dat(p_adt,1,pecell,1,"double",OP_READ),
                  op_arg_dat(p_res,0,pecell,4,"double",OP_INC),
                  op_arg_dat(p_res,1,pecell,4,"double",OP_INC));

      op_par_loop_bres_calc("bres_calc",bedges,
                  op_arg_dat(p_x,0,pbedge,2,"double",OP_READ),
                  op_arg_dat(p_x,1,pbedge,2,"double",OP_READ),
                  op_arg_dat(p_q,0,pbecell,4,"double",OP_READ),
                  op_arg_dat(p_adt,0,pbecell,1,"double",OP_READ),
                  op_arg_dat(p_res,0,pbecell,4,"double",OP_INC),
                  op_arg_dat(p_bound,-1,OP_ID,1,"int",OP_READ));

      //    update flow field
      op_par_loop_update("update",cells,
                  op_arg_dat(p_qold,-1,OP_ID,4,"double",OP_READ),
                  op_arg_dat(p_q,-1,OP_ID,4,"double",OP_WRITE),
                  op_arg_dat(p_res,-1,OP_ID,4,"double",OP_RW),
                  op_arg_dat(p_adt,-1,OP_ID,1,"double",OP_READ)); 
    }
    #endif
  }
  op_timers(&cpu_t2, &wall_t2);
  rms = 0.0;
  op_par_loop_update1("update1",cells,
              op_arg_dat(p_qold,-1,OP_ID,4,"double",OP_READ),
              op_arg_dat(p_q,-1,OP_ID,4,"double",OP_WRITE),
              op_arg_dat(p_res,-1,OP_ID,4,"double",OP_RW),
              op_arg_dat(p_adt,-1,OP_ID,1,"double",OP_READ),
              op_arg_gbl(&rms,1,"double",OP_INC));


  // print iteration history
  rms = sqrt(rms / (double)g_ncell);
  
  // if (iter % 100 == 0)
  //   printf(" %d  %10.5e \n", iter, rms);

  // if (iter % 1000 == 0 &&
  double diff = 0.0;
  if (niter % 1000 == 0 &&
      g_ncell == 720000) { // defailt mesh -- for validation testing
    // printf(" %d  %3.16f \n",iter,rms);
    diff = fabs((100.0 * (rms / 0.0001060114637578)) - 100.0);
    op_printf("\n\nTest problem with %d cells is within %3.15E %% of the "
              "expected solution\n",
              720000, diff);
    
    if (diff < 0.00001) {
      op_printf("This test is considered PASSED\n");
    } else {
      op_printf("This test is considered FAILED\n");
    }
  }

  printf("rank=%d, rms=%.30f, diff=%.30f\n", my_rank, rms, diff);
  
  // output the result dat array to files
  op_print_dat_to_txtfile(p_q, "out_grid_mpi.dat"); // ASCI
  op_print_dat_to_binfile(p_q, "out_grid_mpi.bin"); // Binary

  // write given op_dat's indicated segment of data to a memory block in the
  // order it was originally
  // arranged (i.e. before partitioning and reordering)
  double *q_part = (double *)op_malloc(sizeof(double) * op_get_size(cells) * 4);
  op_fetch_data_idx(p_q, q_part, 0, op_get_size(cells) - 1);
  free(q_part);

  op_timing_output();
  op_printf("Max total runtime = %f\n", wall_t2 - wall_t1);

  op_exit();

  free(cell);
  free(edge);
  free(ecell);
  free(bedge);
  free(becell);
  free(bound);
  free(x);
  free(q);
  free(qold);
  free(res);
  free(adt);
}
