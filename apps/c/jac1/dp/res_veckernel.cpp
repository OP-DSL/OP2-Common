//
// auto-generated by op2.py
//

// user function
inline void res(const double *A, const double *u, double *du,
                const double *beta) {
  *du += (*beta) * (*A) * (*u);
}
#ifdef VECTORIZE
// user function -- modified for vectorisation
void res_vec(const double *A, const double u[*][SIMD_VEC],
             double du[*][SIMD_VEC], const double *beta, int idx) {
  du[0][idx] += (*beta) * (*A) * (u[0][idx]);
}
#endif

// host stub function
void op_par_loop_res(char const *name, op_set set, op_arg arg0, op_arg arg1,
                     op_arg arg2, op_arg arg3) {

  int nargs = 4;
  op_arg args[4];

  args[0] = arg0;
  args[1] = arg1;
  args[2] = arg2;
  args[3] = arg3;
  // create aligned pointers for dats
  ALIGNED_double const double *__restrict__ ptr0 = (double *)arg0.data;
  __assume_aligned(ptr0, double_ALIGN);
  ALIGNED_double const double *__restrict__ ptr1 = (double *)arg1.data;
  __assume_aligned(ptr1, double_ALIGN);
  ALIGNED_double double *__restrict__ ptr2 = (double *)arg2.data;
  __assume_aligned(ptr2, double_ALIGN);

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  op_timing_realloc(0);
  op_timers_core(&cpu_t1, &wall_t1);

  if (OP_diags > 2) {
    printf(" kernel routine with indirection: res\n");
  }

  int exec_size = op_mpi_halo_exchanges(set, nargs, args);

  if (exec_size > 0) {

#ifdef VECTORIZE
#pragma novector
    for (int n = 0; n < (exec_size / SIMD_VEC) * SIMD_VEC; n += SIMD_VEC) {
      if (n + SIMD_VEC >= set->core_size) {
        op_mpi_wait_all(nargs, args);
      }
      ALIGNED_double double dat1[1][SIMD_VEC];
      ALIGNED_double double dat2[1][SIMD_VEC];
#pragma simd
      for (int i = 0; i < SIMD_VEC; i++) {
        int idx1_1 = 1 * arg1.map_data[(n + i) * arg1.map->dim + 1];

        dat1[0][i] = (ptr1)[idx1_1 + 0];

        dat2[0][i] = 0.0;
      }
#pragma simd
      for (int i = 0; i < SIMD_VEC; i++) {
        res_vec(&(ptr0)[1 * (n + i)], dat1, dat2, (double *)arg3.data, i);
      }
      for (int i = 0; i < SIMD_VEC; i++) {
        int idx2_1 = 1 * arg1.map_data[(n + i) * arg1.map->dim + 0];

        (ptr2)[idx2_1 + 0] += dat2[0][i];
      }
      for (int i = 0; i < SIMD_VEC; i++) {
      }
    }

    // remainder
    for (int n = (exec_size / SIMD_VEC) * SIMD_VEC; n < exec_size; n++) {
#else
    for (int n = 0; n < exec_size; n++) {
#endif
      if (n == set->core_size) {
        op_mpi_wait_all(nargs, args);
      }
      int map1idx = arg1.map_data[n * arg1.map->dim + 1];
      int map2idx = arg1.map_data[n * arg1.map->dim + 0];

      res(&(ptr0)[1 * n], &(ptr1)[1 * map1idx], &(ptr2)[1 * map2idx],
          (double *)arg3.data);
    }
  }

  if (exec_size == 0 || exec_size == set->core_size) {
    op_mpi_wait_all(nargs, args);
  }
  // combine reduction data
  op_mpi_set_dirtybit(nargs, args);

  // update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  OP_kernels[0].name = name;
  OP_kernels[0].count += 1;
  OP_kernels[0].time += wall_t2 - wall_t1;
  OP_kernels[0].transfer += (float)set->size * arg1.size;
  OP_kernels[0].transfer += (float)set->size * arg2.size * 2.0f;
  OP_kernels[0].transfer += (float)set->size * arg0.size;
  OP_kernels[0].transfer += (float)set->size * arg3.size;
  OP_kernels[0].transfer += (float)set->size * arg1.map->dim * 4.0f;
}
