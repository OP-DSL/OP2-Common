module save_soln_cuda_module

use OP2_C
use cudaConfigurationParams
use OP2Profiling
use cudafor


  type varSizes_save_soln

    integer(4) :: parg0Size
    integer(4) :: parg1Size

  end type varSizes_save_soln

  ! logical that tells if the input data to the kernel has been already generated
  ! by previous calls to this same op_par_loop function
  logical :: isKernelInputDataGenerated = .false.

  ! sizes of input arguments to CUDA kernel
  type(varSizes_save_soln), device :: argSizes

! real(8), dimension(:), allocatable, device :: argument0
! real(8), dimension(:), allocatable, device :: argument1


  ! input data to CUDA kernel
  ! declared here to make them static variables (i.e. state that survives successive op_par_loop function calls)
  integer(4) :: data0Size, data1Size

contains

  ! subroutine called by the kernel (modified by eliminating c void pointers)
  attributes(device) subroutine save_soln ( q, qold )

    implicit none

    ! declaration of formal parameters
    real(8) :: q(4)
    real(8) :: qold(4)

    ! iteration variable
    integer(4) :: i

    ! size_q and size_qold are the same value
    do i = 1, 4
      qold(i) = q(i)
    end do

  end subroutine save_soln

  ! kernel function
  attributes(global) subroutine op_cuda_save_soln ( argSizes, parg0, parg1, offsetS, setsize, warpSizeOP2 )

    use cudafor

    implicit none

    ! declaration of formal parameters

    type(varSizes_save_soln), device :: argSizes

    real(8), dimension(0:argSizes%parg0Size-1), device :: parg0
    real(8), dimension(0:argSizes%parg1Size-1), device :: parg1

    integer(4), value :: offsetS
    integer(4), value :: setsize
    integer(4), value :: warpSizeOP2

    real(8), dimension(0:3) :: arg0_l
    real(8), dimension(0:3) :: arg1_l

    integer(4) :: tid, offset, nelems, n, m

    ! automatic shared memory
    real(8), shared :: autoshared(0:*)

    integer(4) :: argSDisplacement


    tid = mod ( (threadidx%x)-1, warpSizeOP2 )

    ! remember that:
    ! char *arg_s = shared + offset_s*(threadIdx.x/OP_WARPSIZE);
    ! / 8 is because our autoshared variable is a real(8) variable
    argSDisplacement = (offsetS * ( (threadidx%x-1) / warpSizeOP2 )) / 8

    ! process set elements

    ! implements:
    ! for (int n=threadIdx.x+blockIdx.x*blockDim.x;
    !      n<set_size; n+=blockDim.x*gridDim.x) {
    n = (threadidx%x-1) + (blockidx%x-1) * blockdim%x
    do while ( n .lt. setsize )

      ! implements:
      ! int offset = n - tid;
      offset = n - tid

      ! implements:
      ! int nelems = MIN(OP_WARPSIZE,set_size-offset);
      nelems = min ( warpSizeOP2, (setSize - offset) )

      ! copy data into shared memory, then into local

      !for (int m=0; m<4; m++)
      ! ((double *)arg_s)[tid+m*nelems] = arg0[tid+m*nelems+offset*4];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call ! argSDisplacement
        autoshared ( argSDisplacement  + ( tid + m * nelems ) ) = parg0 ( tid + m * nelems + offset * 4 )

      end do

      ! for (int m=0; m<4; m++)
      !   arg0_l[m] = ((double *)arg_s)[m+tid*4];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        arg0_l(m) = autoshared ( argSDisplacement + ( m + tid * 4 ) )

      end do

      ! user-supplied kernel call

      ! implements:
      ! save_soln( arg0_l,
      !            arg1_l );
      call save_soln ( arg0_l, &
                     & arg1_l &
                   & )

      ! copy back into shared memory, then to device

      ! implements:
      ! for (int m=0; m<4; m++)
      !   ((double *)arg_s)[m+tid*4] = arg1_l[m];

      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        autoshared ( argSDisplacement + ( m + tid * 4 ) ) = arg1_l(m)

      end do


      ! implements:
      ! for (int m=0; m<4; m++)
      !   arg1[tid+m*nelems+offset*4] = ((double *)arg_s)[tid+m*nelems];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        parg1 ( tid + m * nelems + offset * 4 ) = autoshared ( argSDisplacement + ( tid + m * nelems ) )

      end do


      n = n + blockdim%x * griddim%x


    end do

  end subroutine op_cuda_save_soln

  ! caller of the kernel
  attributes(host) function op_par_loop_save_soln ( subroutineName, set, &
                                                  & arg0, idx0, ptr0, access0, &
                                                  & arg1, idx1, ptr1, access1 &
                                                & )

    ! use directives
    use, intrinsic :: ISO_C_BINDING
    use cudafor

    ! mandatory
    implicit none

    ! declaration of intrinsic functions
    intrinsic int, max

    type(profInfo) :: op_par_loop_save_soln

    ! formal arguments
    character, dimension(*), intent(in) :: subroutineName

    ! data set on which we loop
    type(op_set), intent(in) :: set

    ! data ids used in the function
    type(op_dat) :: arg0, arg1

    ! index to be used in first and second pointers
    integer(4), intent(in) :: idx0, idx1

    ! ptr ids for indirect access to data
    type(op_map) :: ptr0, ptr1

    ! access values for arguments
    integer(4), intent(in) :: access0, access1

    ! local variables

    ! used for mallocs and memcpys
!     integer(4) :: data0Size, data1Size

    ! define and compute grid and block sizes and other variables (unused in this case)
    real(8), dimension(:), allocatable, device :: argument0
    real(8), dimension(:), allocatable, device :: argument1

!     type(varSizes_save_soln), device :: argSizes

    integer(4) :: nblocks = 200
    integer(4) :: nthreads = 128
    integer(4) :: nshared = 0
    integer(4) :: offsetS = 0

    integer(4) :: warpSizeOP2

    integer(4) :: threadSynchRet


    ! profiling
    integer :: istat
    type (cudaEvent) :: startKernelTime, stopKernelTime, startHostTime, stopHostTime
    real(4) :: tmpHostTime

    ! create events
    istat = cudaEventCreate(startKernelTime)
    istat = cudaEventCreate(stopKernelTime)
    istat = cudaEventCreate(startHostTime)
    istat = cudaEventCreate(stopHostTime)

    istat = cudaEventRecord ( startHostTime, 0 )


    warpSizeOP2 = OP_WARP_SIZE

    ! this is mandatory, otherwise nshared will become 4096 from the previous invocation!!
    nshared = 0

    ! work out shared memory requirements per element

    nshared = max ( nshared, 8 * 4 ) ! 8 = sizeof(double) => real(8)
    nshared = max ( nshared, 8 * 4 ) ! 8 = sizeof(double) => real(8)


    offsetS = nshared * OP_WARP_SIZE

    nshared = nshared * nthreads


    if ( isKernelInputDataGenerated .eq. .false. ) then

      data0Size = ( arg0%dim * arg0%set%size)
      data1Size = ( arg1%dim * arg1%set%size)


      argSizes%parg0Size = data0Size
      argSizes%parg1Size = data1Size

      isKernelInputDataGenerated = .true.

    end if

    call c_f_pointer ( arg0%dat_d, argument0, (/data0Size/) )
    call c_f_pointer ( arg1%dat_d, argument1, (/data1Size/) )


    istat = cudaEventRecord ( stopHostTime, 0 )
    istat = cudaEventSynchronize ( stopHostTime )
    istat = cudaEventElapsedTime ( tmpHostTime, startHostTime, stopHostTime )

    op_par_loop_save_soln%hostTime = 0
    op_par_loop_save_soln%hostTime = op_par_loop_save_soln%hostTime + tmpHostTime
    tmpHostTime = 0

    istat = cudaEventRecord ( startKernelTime, 0 )

    ! apply kernel to all set elements
    call op_cuda_save_soln<<<nblocks,nthreads,nshared>>> ( argSizes, &
                                                         & argument0, &
                                                         & argument1, &
                                                         & offsetS, &
                                                         & set%size, &
                                                         & warpSizeOP2 &
                                                       & )

    ! synchronise threads after kernel call
    threadSynchRet = cudaThreadSynchronize()

    istat = cudaEventRecord ( stopKernelTime, 0 )
    istat = cudaEventSynchronize ( stopKernelTime )
    istat = cudaEventElapsedTime ( op_par_loop_save_soln%kernelTime, startKernelTime, stopKernelTime )

    istat = cudaEventRecord ( startHostTime, 0 )
    ! empty code here...only if there is a reduction it is filled up with something
    istat = cudaEventRecord ( stopHostTime, 0 )
    istat = cudaEventSynchronize ( stopHostTime )
    istat = cudaEventElapsedTime ( tmpHostTime, startHostTime, stopHostTime )

    op_par_loop_save_soln%hostTime = op_par_loop_save_soln%hostTime + tmpHostTime

  end function op_par_loop_save_soln

end module save_soln_cuda_module
