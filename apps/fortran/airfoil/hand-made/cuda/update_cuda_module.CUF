module update_cuda_module

  use OP2_Fortran_Declarations
  use OP2_Fortran_RT_Support
  use cudaConfigurationParams
! use constantVarsCuda
  use OP2Profiling
  use cudafor

  type varSizes_update

    integer(4) :: parg0Size
    integer(4) :: parg1Size
    integer(4) :: parg2Size
    integer(4) :: parg3Size
    integer(4) :: parg4Size

  end type varSizes_update

  ! debug
  logical :: firstTimeCalled = .true.

  ! logical that tells if the input data to the kernel has been already generated
  ! by previous calls to this same op_par_loop function
  logical :: isKernelInputDataGenerated = .false.

  type(varSizes_update), device :: argSizes

  integer(4) :: arg0Size, arg1Size, arg2Size, arg3Size, arg4Size

  real(8), dimension(:), allocatable :: redArrayHost
  real(8), dimension(:), allocatable, device :: redArrayDev

! real(8), dimension(:), pointer :: c2fPtrArg4

contains

  ! implements global tree reduction for OP_INC only
  attributes(device) subroutine reduct_update ( dat_g, dat_l, wsize, startOffset )

    use cudafor

    implicit none

    real(8), dimension(1), device :: dat_g
    real(8), value :: dat_l

    ! this is the value of warpsize, passed from the caller because I can't use it here
    ! (I don't know why, the compiler returns an internal error)
    integer, value :: wsize

    ! This is the starting offset in the automatically allocated shared variable.
    ! The space following this index is reserved for use to this reduction routine
    integer(4), value :: startOffset

    real(8), dimension(0:*), shared :: temp
    integer(4) :: tid

    integer(4) :: d

    ! must convert the starting offset from byte to real(8) element addressing
    startOffset = startOffset / 8

    ! the size can be precomputed without the need of automatically allocate it
    tid = ( threadidx%x - 1 )

    ! the following is a right shift of 1 bit
    d = ishft ( blockdim%x, -1 )

    call syncthreads()

    temp(startOffset + tid) = dat_l

    ! implements:
    ! for (; d>warpSize; d>>=1) {
    do while ( d .gt. 0 )

      call syncthreads()

      if ( tid .lt. d ) then
        ! in this case it just implements OP_INC
        temp(startOffset + tid ) = temp(startOffset + tid ) + temp(startOffset + (tid + d) )
      end if

      d = ishft ( d, -1 )
    end do

    call syncthreads()

    ! the first thread in each block terminates the local reduction
    if ( tid .eq. 0 ) then

      ! in this case it just implements OP_INC
      dat_g(1) = dat_g(1) + temp(startOffset)

    end if

    call syncthreads()

  end subroutine reduct_update

  attributes(device) subroutine update ( qold, q, res, adt, rms )

    implicit none

    ! formal parameters
    real(8), dimension(4) :: qold
    real(8), dimension(4) :: q
    real(8), dimension(4) :: res
    real(8), dimension(1) :: adt
    real(8), dimension(1) :: rms

    real(8) :: del, adti

    integer(4) :: i

    adti = 1.0 / adt(1)

    do i = 1, 4
      del = adti * res(i)
      q(i) = qold(i) - del
      res(i) = 0.0
      rms(1) = rms(1) + del * del
    end do

  end subroutine update

  attributes(global) subroutine op_cuda_update ( argSizes, &
                                               & parg0, &
                                               & parg1, &
                                               & parg2, &
                                               & parg3, &
                                               & parg4, &
                                               & offsetS, &
                                               & setSize, &
                                               & warpSizeOP2, &
                                               & redStartOffset &
                                             & )
    use cudafor

    implicit none

    type(varSizes_update), device :: argSizes

    real(8), dimension(0:(argSizes%parg0Size)-1), device :: parg0
    real(8), dimension(0:(argSizes%parg1Size)-1), device :: parg1
    real(8), dimension(0:(argSizes%parg2Size)-1), device :: parg2
    real(8), dimension(0:(argSizes%parg3Size)-1), device :: parg3
    real(8), dimension(0:(argSizes%parg4Size)-1), device :: parg4


    integer(4), value :: offsetS
    integer(4), value :: setSize
    integer(4), value :: warpSizeOP2

    ! the following is the starting position in the automatically allocated shared memory
    ! reserved for using by the reduction routine
    integer(4), value :: redStartOffset

    real(8), dimension(0:3) :: arg0_l
    real(8), dimension(0:3) :: arg1_l
    real(8), dimension(0:3) :: arg2_l
    real(8), dimension(0:0) :: arg4_l

    ! iteration variables
    integer(4) :: d, n, m
    integer(4) :: tid, offset, nelems, outof

    ! debug
!   integer(4) :: outofbound = 0

    ! automatic shared memory
    real(8), shared :: autoshared(0:*)

    ! remember from now on that:
    ! char *arg_s = shared + offset_s*(threadIdx.x/OP_WARPSIZE);
    integer(4) :: argSDisplacement

    ! the displacement must be given in terms of real(8) elements, and not shared variables
    argSDisplacement = (offsetS * ( (threadidx%x-1) / warpSizeOP2 )) / 8

    do d = 0, 0

      arg4_l(d) = 0

    end do

    ! implements:
    ! int   tid = threadIdx.x%OP_WARPSIZE;
    tid = mod ( (threadidx%x)-1, warpSizeOP2 )

    ! process set elements

    ! implements:
    ! for (int n=threadIdx.x+blockIdx.x*blockDim.x;
    !      n<set_size; n+=blockDim.x*gridDim.x) {
    n = (threadidx%x-1) + (blockidx%x-1) * blockdim%x
    do while ( n .lt. setSize )

      offset = n - tid

      nelems = min ( warpSizeOP2, (setSize - offset) )

      ! copy data into shared memory, then into local

      ! implements:
      ! for (int m=0; m<4; m++)
      !   ((double *)arg_s)[tid+m*nelems] = arg0[tid+m*nelems+offset*4];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call ! argSDisplacement
        autoshared ( argSDisplacement  + ( tid + m * nelems ) ) = parg0 ( tid + m * nelems + offset * 4 )

      end do

      ! implements:
      ! for (int m=0; m<4; m++)
      !   arg0_l[m] = ((double *)arg_s)[m+tid*4];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        arg0_l(m) = autoshared ( argSDisplacement + ( m + tid * 4 ) )

      end do

      ! implements:
      ! for (int m=0; m<4; m++)
      !   ((double *)arg_s)[tid+m*nelems] = arg2[tid+m*nelems+offset*4];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        autoshared ( argSDisplacement + ( tid + m * nelems ) ) = parg2 ( tid + m * nelems + offset * 4 )

      end do

      ! implements:
      ! for (int m=0; m<4; m++)
      !   arg2_l[m] = ((double *)arg_s)[m+tid*4];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        arg2_l(m) = autoshared ( argSDisplacement + ( m + tid * 4 ) )

      end do

      ! user-supplied kernel call

      ! implements:
      ! update( arg0_l,
      !         arg1_l,
      !         arg2_l,
      !         arg3+n,
      !         arg4_l );

      call update ( arg0_l, &
                  & arg1_l, &
                  & arg2_l, &
                  & parg3 ( n : n ), &
                  & arg4_l  &
                & )

      ! copy back into shared memory, then to device

      ! implements:
      ! for (int m=0; m<4; m++)
      !   ((double *)arg_s)[m+tid*4] = arg1_l[m];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        autoshared ( argSDisplacement + ( m + tid * 4 ) ) = arg1_l(m)

      end do

      ! implements:
      ! for (int m=0; m<4; m++)
      !   arg1[tid+m*nelems+offset*4] = ((double *)arg_s)[tid+m*nelems];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        parg1 ( tid + m * nelems + offset * 4 ) = autoshared ( argSDisplacement + ( tid + m * nelems ) )

      end do

      ! implements:
      ! for (int m=0; m<4; m++)
      !   ((double *)arg_s)[m+tid*4] = arg2_l[m];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        autoshared ( argSDisplacement + ( m + tid * 4 ) ) = arg2_l(m)

      end do

      ! implements:
      ! for (int m=0; m<4; m++)
      !   arg2[tid+m*nelems+offset*4] = ((double *)arg_s)[tid+m*nelems];
      do m = 0, 3

        ! autoshared ( argSDisplacement ) = arg_s
        ! 4 is the dimension of argument 0 in this op_par_loop call
        parg2 ( tid + m * nelems + offset * 4 ) = autoshared ( argSDisplacement + ( tid + m * nelems ) )

      end do

      n = n + blockdim%x * griddim%x

    end do

    ! global reductions

    ! implements:
    ! for(int d=0; d<1; d++) op_reduction<OP_INC>(&arg4[d+blockIdx.x*1],arg4_l[d]);

    do d = 0, 0

      call reduct_update ( parg4 ( d + (blockidx%x -1) * 1 : d + (blockidx%x -1) * 1 ), arg4_l(d), warpSizeOP2, redStartOffset )

    end do

!   if ( outof .eq. -1 ) then
!
!     parg4 ( 0 + (blockidx%x -1) * 1 ) = -100.0
!
!   else
!
!     parg4 ( 0 + (blockidx%x -1) * 1 ) = -200.0
!
!   end if

  end subroutine op_cuda_update

! attributes(host) function round_up ( intval )
!
!   integer(4) :: intval
!
!   integer(4) :: round_up
!
!   round_up = iand ( ( intval ) + 15, not ( 15 ) )
!
! end function round_up

  attributes(host) function op_par_loop_update ( subroutineName, setIn, &
                                                 & arg0In,   idx0, map0In, access0, &
                                                 & arg1In,   idx1, map1In, access1, &
                                                 & arg2In,   idx2, map2In, access2, &
                                                 & arg3In,   idx3, map3In, access3, &
                                                 & arg4In,   idx4, map4In, access4  &
                                               & )
    use cudafor

    implicit none

    type(profInfo) :: op_par_loop_update

    ! formal arguments
    character(kind=c_char,len=*), intent(in) :: subroutineName

    ! data set on which we loop
    type(op_set), intent(in) :: setIn

    ! data ids used in the function
    type(op_dat) :: arg0In, arg1In, arg2In, arg3In, arg4In

    ! index to be used in first and second pointers
    integer(4), intent(in) :: idx0, idx1, idx2, idx3, idx4

    ! ptr ids for indirect access to data
    type(op_map) :: map0In, map1In, map2In, map3In, map4In

    ! access values for arguments
    integer(4), intent(in) :: access0, access1, access2, access3, access4

    ! local variables
    type(op_set_core), pointer :: set
    type(op_map_core), pointer :: map0, map1, map2, map3, map4
    type(op_dat_core), pointer :: arg0, arg1, arg2, arg3, arg4

    ! set CUDA execution parameters
    integer(4) :: nblocks = 200
    integer(4) :: nthreads = 128

    ! transfer global reduction data to GPU
    integer(4) :: maxblocks
    integer(4) :: reductItems = 0
    integer(4) :: reductSize = 0

    ! these two variables implement the global reduction in the host and device memory, respectively
    ! and they contain the values computed by each thread block

    type(op_set_core), pointer :: arg0Set, arg1Set, arg2Set, arg3Set
!   integer(4) :: arg0Size, arg1Size, arg2Size, arg3Size, arg4Size
    real(kind=c_double), dimension(:), pointer :: c2fPtrArg4

    real(8), dimension(:), allocatable, device :: argument0
    real(8), dimension(:), allocatable, device :: argument1
    real(8), dimension(:), allocatable, device :: argument2
    real(8), dimension(:), allocatable, device :: argument3
!   real(8), dimension(:), allocatable, device :: argument4

    ! this is needed to avoid having more than 1 device variable in the same expression (see re-assignement
    ! after kernel call below)
    real(8) :: partialArg4Sum = 0
    real(8) :: partialRedArrayDev4Sum = 0

    ! iteration variables
    integer(4) :: i, b, d

    ! CUDA configuration variables and related stuff
    integer(4) :: nshared
    integer(4) :: offsetS
    integer(4) :: warpSizeOP2
    integer(4) :: maxReductionSize
    integer(4) :: redStartOffset

    ! value returned by thread synchronisation function
    integer(4) :: threadsynchret

    ! size of arguments
!   type(varSizes_update), device :: argSizes

    real(8) :: reductedValue = 0

    integer(4) :: retDebug
    real(8) :: datad, summed

    ! profiling
    integer :: istat
    type (cudaEvent) :: startKernelTime, stopKernelTime, startHostTime, stopHostTime
    real(4) :: tmpHostTime

    ! create events
    istat = cudaEventCreate(startKernelTime)
    istat = cudaEventCreate(stopKernelTime)
    istat = cudaEventCreate(startHostTime)
    istat = cudaEventCreate(stopHostTime)

    istat = cudaEventRecord ( startHostTime, 0 )

    ! initialising arguments
    set => setIn%setPtr

    map0 => map0In%mapPtr
    map1 => map1In%mapPtr
    map2 => map2In%mapPtr
    map3 => map3In%mapPtr
    map4 => map4In%mapPtr

    arg0 => arg0In%dataPtr
    arg1 => arg1In%dataPtr
    arg2 => arg2In%dataPtr
    arg3 => arg3In%dataPtr
    arg4 => arg4In%dataPtr

    call c_f_pointer ( arg0%set, arg0Set )
    arg0Size = arg0%dim * arg0set%size

    call c_f_pointer ( arg1%set, arg1Set )
    arg1Size = arg1%dim * arg1set%size

    call c_f_pointer ( arg2%set, arg2Set )
    arg2Size = arg2%dim * arg2set%size

    call c_f_pointer ( arg3%set, arg3Set )
    arg3Size = arg3%dim * arg3set%size

    ! warning: the argument is a global op_dat so the size is directly the dim of the op_dat
    arg4Size = arg4%dim

    warpSizeOP2 = OP_WARP_SIZE

    maxblocks = nblocks

    ! 1 is the size of p_rms and 8 is the size of its type (real(8))
    ! Mike's version is : reductBytes + round_up ( maxblocks * 1 * 8 )

    reductItems = maxblocks * 1 ! 1 is the dimension of the global data

    reductSize = max ( reductSize, 8 )

    if ( isKernelInputDataGenerated .eq. .false. ) then

      isKernelInputDataGenerated = .true.

      ! setting up arguments sizes
      argSizes%parg0Size = arg0Size
      argSizes%parg1Size = arg1Size
      argSizes%parg2Size = arg2Size
      argSizes%parg3Size = arg3Size

      ! the size of the last argument is different from the one of the global data (arg4),
      ! because we instatiate a copy of that global data on device memory for each thread used
      argSizes%parg4Size = reductItems

      ! the first time allocate enough space on host and device for reduction
      allocate ( redArrayHost ( reductItems ) )
      allocate ( redArrayDev ( reductItems ) )

    end if

    ! work out shared memory requirements per element (this is mandatory, otherwise the default value is the
    ! one that has been assigned as a result of the previous execution of this op_par_loop)
    nshared = 0

    ! this should be done for each op_dat argument copied to shared memory
    ! in this case the three of them have dim = 4 and it is actually useless
    nshared = max ( nshared, 8 * 4 ) ! 8 = sizeof(double) => real(8)
    nshared = max ( nshared, 8 * 4 ) ! 8 = sizeof(double) => real(8)
    nshared = max ( nshared, 8 * 4 ) ! 8 = sizeof(double) => real(8)

    offsetS = nshared * OP_WARP_SIZE

    ! in Fortran the reduction variable is merged in the same automatically allocated
    ! shared variable, and the size is: the size of the needed memory for the
    ! arguments *plus* the size of the needed memory for the reduction variable (see below)
    nshared = nshared * nthreads

!   nshared = max ( nshared * nthreads, reductSize * nthreads )

    ! 1. transfer C pointers to Fortran pointers

    call c_f_pointer ( arg0%dat_d, argument0, (/arg0Size/) )
    call c_f_pointer ( arg1%dat_d, argument1, (/arg1Size/) )
    call c_f_pointer ( arg2%dat_d, argument2, (/arg2Size/) )
    call c_f_pointer ( arg3%dat_d, argument3, (/arg3Size/) )

    ! we conver the dat (host) and not dat_d (device) because this is a global data
    call c_f_pointer ( arg4%dat, c2fPtrArg4, (/arg4Size/) )

    ! in Fortran CUDA we have to allocate enough automatic shared memory
    ! directly from here, and to fit all information in the "same space:
    ! (see the difference between Fortran CUDA automatic shared variables
    ! and C CUDA ones)

    ! the start offset of dynamically allocated shared memory is the end of the previously
    ! allocated shared space used for some of the parallel loop variables
    redStartOffset = nshared

    ! Adding up memory space for shared variable in reduction routine
    ! (this is actually the max addressable position in the reduction variable)
    maxReductionSize = nthreads * 8 !( nthreads - 1 ) +  ishft ( nblocks, -1 ) * reductSize
    nshared = nshared + maxReductionSize

    istat = cudaEventRecord ( stopHostTime, 0 )
    istat = cudaEventSynchronize ( stopHostTime )
    istat = cudaEventElapsedTime ( tmpHostTime, startHostTime, stopHostTime )

    op_par_loop_update%hostTime = 0
    op_par_loop_update%hostTime = op_par_loop_update%hostTime + tmpHostTime
    tmpHostTime = 0

    istat = cudaEventRecord ( startKernelTime, 0 )
! moved here to reflect Mike's timing

    ! always zero out reduction variable on host
    do i = 1, reductItems

      redArrayHost ( i ) = 0

    end do

    ! and then copy it to device
    redArrayDev = redArrayHost

    call  op_cuda_update <<< nblocks, nthreads, nshared >>> ( argSizes, &
                                                            & argument0, &
                                                            & argument1, &
                                                            & argument2, &
                                                            & argument3, &
                                                            & redArrayDev, &
                                                            & offsetS, &
                                                            & set%size, &
                                                            & warpSizeOP2, &
                                                            & redStartOffset &
                                                          & )

    ! synchronise threads after kernel call
    threadSynchRet = cudaThreadSynchronize()

    ! copy back reduction variable, from device to host
    redArrayHost = redArrayDev

!    print *, 'printing out host array'
!    summed = 0
!    do i = 1, reductItems

!      print *, redArrayHost ( i )
! summed = summed + redArrayHost ( i )

!    end do
!print *, summed
!    stop

    ! 2. reduct the host array to the single user variable
    ! implements:

    ! for (int b=0; b<maxblocks; b++)
    !   for (int d=0; d<1; d++)
    !     arg4h[d] = arg4h[d] + ((double *)arg4.dat)[d+b*1];
    do b = 1, reductItems

      do d = 0, 0

            c2fPtrArg4 ( d + 1 ) = c2fPtrArg4 ( d + 1 ) + redArrayHost ( d + b * 1 )

      end do

    end do

! moved here to reflect Mike's timing...

    istat = cudaEventRecord ( stopKernelTime, 0 )
    istat = cudaEventSynchronize ( stopKernelTime )
    istat = cudaEventElapsedTime ( op_par_loop_update%kernelTime, startKernelTime, stopKernelTime )

    istat = cudaEventRecord ( startHostTime, 0 )

    istat = cudaEventRecord ( stopKernelTime, 0 )
    istat = cudaEventSynchronize ( stopKernelTime )
    istat = cudaEventElapsedTime ( op_par_loop_update%kernelTime, startKernelTime, stopKernelTime )

    istat = cudaEventRecord ( startHostTime, 0 )
    istat = cudaEventRecord ( stopHostTime, 0 )
    istat = cudaEventSynchronize ( stopHostTime )
    istat = cudaEventElapsedTime ( tmpHostTime, startHostTime, stopHostTime )

    op_par_loop_update%hostTime = op_par_loop_update%hostTime + tmpHostTime

  end function op_par_loop_update

end module update_cuda_module

