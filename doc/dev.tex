\documentclass[11pt]{article}
\usepackage[colorlinks,urlcolor=blue,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{graphicx}
\usepackage{verbatim}
\setlength{\oddsidemargin}{-0.01in}
\setlength{\topmargin}{-0.4in}
\setlength{\textheight}{9.0in}
\setlength{\textwidth}{6.5 in}
\usepackage{url}
\usepackage{verbments}

\date{December 2013}

\parskip 5pt


\usepackage{algorithm}
\usepackage{algorithmic}

%\newcommand{\theHalgorithm}{\arabic{algorithm}}

\newenvironment{alg}[2]
{
\vspace{-0.5cm}
\begin{flushleft}
\begin{minipage}[c]{0.8\linewidth}
\begin{algorithm}[H]
\caption{#1}
\label{#2}
\begin{algorithmic}
}
{
\end{algorithmic}
\end{algorithm}
\end{minipage}
\end{flushleft}
}



\begin{document}

\title{OP2 Developers Guide}
\author{Mike Giles}

\maketitle

\begin{abstract}
This document explains some of the algorithms and implementation
details inside OP2.  It is intended primarily for those who are
developing OP2; those who are only using OP2 should instead read
the Users Manual.
\vspace*{0.2in}
\end{abstract}

\newpage

\tableofcontents

\newpage

\section{Overall parallelisation strategy}

The OP2 design uses hierarchical parallelism with two principal levels.

At the higher level, OP2 is parallelized across distributed-memory
clusters using MPI message-passing.  This uses essentially the same
implementation approach as the original OPlus.  The domain is
partitioned among the compute nodes of the cluster, and import/export
halos are constructed for message-passing.  Data conflicts when
incrementing indirectly referenced datasets are avoided by using an
``owner-compute'' model, in which each process performs the computations
which are required to update data owned by that partition.  This approach
involves some amount of redundant computation; for example, if the
computation for a particular edge results in increments to the data at
two nodes belonging to different partitions, then both of those partitions
will need to perform this edge computation.  However, there will be a
minimal level of redundant computation if the cluster satisfies the first
of our key assumptions:

\vspace{0.1in}
\noindent {\bf Key assumption 1: each compute node will have GBs of memory}
\vspace{0.1in}

\noindent At the lower level, there are different kinds of parallelism depending
on the target hardware:
\begin{itemize}
\item
on NVIDIA GPUs, the lower level is split into two, with multiple
thread blocks, and multiple threads within each block;
\item
on CPUs, the lower level may also be split in two, with shared-memory
multithreading through OpenMP, and a possibility of each thread using
vectorization to exploit the capabilities of SSE/AVX vector units.
\end{itemize}

\noindent In both cases, we have two further key assumptions:

\vspace{0.1in}
\noindent {\bf Key assumption 2: memory bandwidth is a major limitation}
\vspace{0.1in}

\noindent In the case of GPUs, this refers to the bandwidth between the main
graphics memory and the GPU, whereas for CPUs it is the bandwidth between
the main system memory and the CPUs.

\vspace{0.1in}
\noindent {\bf Key assumption 3: there is very little local shared memory}
\vspace{0.1in}

\noindent In the case of GPUs, this refers to shared memory with a SM
(Streaming Multiprocessor) unit of the GPU, whereas for CPUs it
refers to the size of the L1 cache.\\


\noindent These assumptions will motivate the implementation design decisions.




\newpage

\section{CUDA parallelisation strategy}

Because of Key Assumption 2, the starting point in designing the
CUDA implementation is to minimize the amount of data which needs
to be transferred between the graphics memory and the GPU.

There are two ways to do this; through explicit staging in shared memory
which is used when generating code for Fermi-generation GPUs, or by
relying on caches (texture and L2), which is used for Kepler GPUs.

For indirect loops (loops which involve indirectly-referenced datasets)
this leads to the idea of working with mini-partitions which are small
enough so that all of the indirect datasets for each of the mini-partitions
is able to fit into the limited shared memory on each SM within an NVIDIA
GPU.

Read-only (and read-write) indirect data is loaded in once at the beginning
of a CUDA kernel and then re-used as needed by the threads working within
the thread block.  Write-only (and read-write) indirect data is written
back to graphics memory at the end of a kernel.

For data which is incremented, the shared memory array of increments is
initialized to zero, and then at the end of the kernel the increments are
applied to the global array held in the graphics memory.  In doing this,
we need to avoid the potential data conflicts (race conditions).
For example, in the ``airfoil'' testcase in the OP2 distribution,
different edges in the {\tt res} routine can update data at the same
cell.  The problems are avoided by coloring
\footnote{This was a well-established technique for vector computing.}
at two levels:
\begin{itemize}
\item
the blocks are colored so that there is no data conflict between two
blocks of the same color -- a separate CUDA kernel call is used for each
block color, with a synchronization between colors to ensure there is no
conflict;
\item
the threads within each block are colored, and the increments are applied
one color at a time, with a {\tt \_\_syncthreads} thread synchronization
between each thread color -- this results in some loss of performance due
to CUDA warp divergence (i.e.~a lot of threads are idle during this
incrementing process) but overall it is not large.
\footnote{To minimize the number of colors within each thread ``warp'',
it may be best to reorder the threads within each block.  This would
scramble up the identity mapping used for directly-referenced data,
so there would be some communication cost, but it's possible there would
be savings overall.}
\footnote{
An alternative to thread coloring would be for each thread to store its
increment separately in the shared memory, and then have an additional loop
within the kernel in which the increments for each data element are combined
by a unique thread.  This approach has been used for a finite element
application by C.~Cecka at Stanford, but it requires a lot of shared memory.
Because of Key Assumption 3, this would lead to extremely small
mini-partitions, which in turn would give very poor data reuse, and so
we chose not to try this approach.}

\end{itemize}


\subsection{Execution plans}

In standard MPI computations, because of the cost of re-partitioning,
the partitioning is usually done just once and the same partitioning is
then used for each stage of the computation.

In contrast, between each stage of the computation on the GPU the data
resides in the main graphics memory, and so the blocking for each parallel
loop calculation can be considered independently of the requirements of
the other parallel loops.

Based on ideas from FFTW, we construct for each parallel loop an execution
``plan'' which is a customized blocking of the execution on the GPU
for that loop, making optimum use of the local shared memory on each
multiprocessor considering in detail the memory requirements of the loop
computation.

\subsection{Renumbering}

The execution plans construct mini-partitions by breaking the set being
looped over into a number of contiguous blocks. This is good for direct
datasets since they will be contiguous and it will lead to coalesced
memory transfers.  However, for indirect datasets we want to make sure
that each mini-partition references elements which are held close together
in the graphics memory.

An initial step (which has not yet been implemented: see the ``To Do list''
at the end of this document) is therefore to reorder/renumber all sets
to improve data locality, so that neighboring nodes or edges have
indices, and therefore memory locations, which are close to each other.
There are lots of ways in which this might be done, but one simple
effective way which we have used in the past is recursive coordinate
bisection.
%For a 3D dataset, one would first bisect each set in the $x$-direction,
%then the $y$-direction, then the $z$-direction, and then repeat the
%sequence recursively to build up a binary tree whose leaves are then
%numbered sequentially.

\subsection{Some rough numbers for HYDRA}

A big calculation by the Rolls-Royce HYDRA CFD code will use most
of the 6GB on a C2070 card. Given that the local shared-memory size
is 48kB, this suggests that the data intensive parallel loops will have
over 100,000 blocks.
%\footnote{On the one hand, this will be an underestimate
%since some of the data will be used by more than one block,
%but on the other hand it will be an overestimate
%because not all of the data will be needed for just one loop.}
10 block colors might be needed to avoid data conflicts, suggesting
up to 10,000 blocks per color.
%Given this number it doesn't matter if there are variations in
%the amount of compute per block; the important thing is to make
%sure each block uses as much shared memory as possible.

HYDRA needs up to 40 floats per grid node.  16kB corresponds
to 4000 floats which is equivalent to 100 nodes, which is
roughly $5^3$.  This should be big enough to get a fair degree
of re-use of nodal data within the block, maybe 50\% of
maximum.  The block may have 400 edges, with maybe 40 per
color so we need to use color-locked increments, with thread
synchronization in between colors.

%A typical big CFD calculation (such as Rolls-Royce's HYDRA code)
%may need 50-200 plans each of which may be executed more than once
%during a single timestep or multigrid iteration.

\newpage

\section{Plan construction}

This section deals with the algorithms in routine {\tt \bf op\_plan\_core}
within the file {\tt \bf op\_lib\_core.c}.  This is called whenever a
parallel loop has at least one indirect dataset.

\subsection{indirect datasets}

\label{sec:indirect_datasets}

``Sets'' are things like nodes or edges, a collection of abstract
``elements'' over which the parallel loops execute.
``Datasets'' are the data associated with the sets, such as flow
variables or edge weights, which are the arguments to the parallel
loop functions.

In a particular parallel loop, an ``indirect dataset'' is one which
is referenced indirectly using a mapping from another set.  Note that
more than one argument of the parallel loop can address the same
indirect dataset.  For example, in a typical CFD edge flux calculation,
two arguments will correspond to the flow variables belonging to the
nodes at either end of the edge, and another two arguments will
correspond to the flux residuals at either end.

The pre-processor {\tt \bf op2.m} identifies for each parallel loop
the number of arguments {\tt nargs}, the number of indirect datasets
{\tt ninds} and the mapping from arguments to indirect datasets
{\tt inds[]}.   The last of these has an entry for each argument;
if it is equal to $-1$ then the argument does not reference an
indirect dataset.
All of this information is supplied as input to the routine
{\tt \bf op\_plan\_core}.

\subsection{local renumbering}

The execution plan divides the execution set into mini-partitions.
These are referred to in the code as ``blocks'' because it's a
shorter word. This is a slightly different use of the word ``block''
compared to CUDA thread blocks, but each plan block is worked on by
a single CUDA block so it's hopefully not too confusing.

The plan blocks are sized so that the indirect datasets will fit
within the limited amount of shared memory available to each SM
(``streaming multiprocessor'', NVIDIA's preferred term to describe
each of the execution units in their GPUs).  The idea is that the
indirect datasets are held within the shared memory to maximize data
reuse and avoid global memory traffic.  However this requires
renumbering of the mappings used to reference these datasets.

%\newpage

For each plan block, and each indirect dataset within it, the
algorithm for the renumbering is:
\begin{itemize}
\item
build a list of all references to the dataset by simply appending
to a list
\item
sort the list and eliminate duplicates -- this then defines the
mapping from local indices to global indices
\item
use a large work array to invert the mapping, to give the mapping
from global indices to local indices (note: this is obviously only
needed for the global indices occurring within that block)
\item
create a new copy of the mapping table which uses the new local
indices
\end{itemize}

Note that each indirect dataset ends up with its own duplicate
mapping table.  In some cases, the indirect datasets had the same
original mapping tables; for example, in the CFD edge flux loop
described before the flow variables and flux residuals were
referenced using the same edge-node mappings.  In this case,
we are currently wasting both memory and memory bandwidth by
duplicating the renumbered mapping tables.  This should be
eliminated in the future, by identifying such duplication,
de-allocating the duplicates, and changing the pointer to the
duplicate table to point to the primary table.

Note also that for multi-dimensional mappings one has to use the
appropriate mapping index as specified in the inputs, and the
re-numbered mappings which are stored are for that index alone.

%\newpage

\subsection{coloring}

Coloring is used at two levels to avoid data conflicts.  The
elements within each block are colored, and then the blocks
themselves are colored.

We start by describing the element coloring. The goal is
to assign a color to each element of that no two elements
of the same color reference the same indirect dataset element.

Conceptually, for each indirect dataset element we maintain
a list of the colors of the elements which reference it.
Starting with this list initialized to be empty, the mathematical
algorithm treats each set element in turn and performs the
following steps:
\begin{itemize}
\item
loop over all indirect dataset elements referenced by the set
element to find the lowest index color which does not already
reference them
\item
set this to be the color of the element
\item
loop again over all indirect datasets, adding this color
to their list
\end{itemize}

The efficient implementation of this uses bit operations.
The color list for each indirect dataset element is a
32-bit integer in a work array, with the $i^{th}$ bit set
to 1 if it is referenced by an element of color $i$.
The first step is performed by using the bit-wise {\tt or}
operation to combine the color lists into a variable called
{\tt mask}, followed by using the {\tt ffs} instruction to
find the first zero bit.  The third step is also performed
by a bit-wise {\tt or} operation.

Doing it in this way, we can process up to 32 colors in a
single pass.  This is probably sufficient for most applications,
but when it is not, the code loops back. i.e.~in the first pass,
if the first step finds that all bits are already set, it doesn't
assign a color to the element, and goes on to the next element.
Then at the end it goes back to process the elements which have
not yet been colored, with the color lists re-initialized to
indicate that the indirect set elements are not referenced by
any of the ``new'' colors.  This outer loop (controlled by the
variable {\tt repeat}) is repeated until all elements have been
colored.

The block coloring is performed in exactly the same way, except
that in the first and third steps the loop is over all indirect
dataset elements referenced by all of the elements in the block,
not just by a single element.

%\newpage

\begin{figure}
\begin{center}
{\setlength{\unitlength}{0.23in}\begin{picture}(20,5.5)(-1,0)

\multiput(0,4)(1,0){20}{\framebox(1,1){}}
\multiput(0,0)(1,0){20}{\framebox(1,1){}}

\multiput( 0,4)(1,0){5}{\framebox(1,1){0}}
\multiput( 5,4)(1,0){5}{\framebox(1,1){1}}
\multiput(10,4)(1,0){5}{\framebox(1,1){2}}
\multiput(15,4)(1,0){5}{\framebox(1,1){3}}

\multiput( 0,0)(4,0){5}{\framebox(1,1){0}}
\multiput( 1,0)(4,0){5}{\framebox(1,1){1}}
\multiput( 2,0)(4,0){5}{\framebox(1,1){2}}
\multiput( 3,0)(4,0){5}{\framebox(1,1){3}}

\put(0.5,4){\vector(0,-1){3}}
\multiput(4.5,1)(4,0){4}{\vector(0,-1){0}}
\multiput(1.5,1)(4,0){3}{\vector(0,-1){0}}

\put(3,1){\oval(3,2)[tr]}
\put(4,1){\oval(9,3)[tr]}
\put(5,1){\oval(15,4)[tr]}
\put(6,1){\oval(21,5)[tr]}

\put(3,4){\oval(3,4)[bl]}
\put(4,4){\oval(3,3)[bl]}
\put(5,4){\oval(3,2)[bl]}
\put(6,4){\oval(3,1)[bl]}

\put(3,1){\oval(3,1)[tl]}
\put(3,4){\oval(5,5)[br]}

\put(6,1){\oval(1,1)[tl]}
\put(6,4){\oval(1,5)[br]}

\put(8,1){\oval(3,1)[tr]}
\put(8,4){\oval(1,5)[bl]}

\put(-0.5,0.5){\makebox(0,0)[r]{blocks}}
\put(-0.5,4.5){\makebox(0,0)[r]{blkmap}}

\end{picture}}
\end{center}

\caption{Illustration of block mapping, with colors indicated as 0, 1, etc.}
\label{fig:blkmap}
\end{figure}

\subsection{block mapping}

\label{sec:block_mapping}

The final part of {\tt \bf op\_plan\_core} defines a block mapping.
As illustrated in the bottom row of Fig.~\ref{fig:blkmap},
{\tt \bf op\_plan\_core} constructs blocks and stores their data in
the order in which they are generated, so they are not
grouped by color.

Rather than reordering the blocks to group them by color,
I instead construct the {\tt blkmap} mapping from a grouped
arrangement to the actual blocks.  This, together with the
number of blocks of each color, is all that is needed for
the later kernel execution.

The algorithm to compute {\tt blkmap} is:
\begin{itemize}
\item
compute the total number of blocks of each color
\item
do a cumulative summation to obtain the sum
of all blocks of preceding colors
\item
processing each block in turn, add the number of preceding
blocks of the same color to the cumulative sum of
preceding colors, to obtain its position in the {\tt blkmap}
array
\item
finally, undo the cumulative summation operation to store
the number of blocks of each color
\end{itemize}


\subsection{rest}

The first part of {\tt \bf op\_plan\_core} checks whether there is an
existing plan to deal with this parallel loop, and if not it does some
self-consistency checking.

The final part of {\tt \bf op\_plan\_core} computes the maximum amount
of shared memory required by any of the blocks, and copies the plan arrays
over onto the GPU, keeping the pointers in the {\tt op\_plan\_core}
structure.


\subsection{op\_plan struct}

The first part of the {\tt \bf op\_plan} struct stores the input arguments
used to construct the plan. These are needed to determine whether the inputs
for a new parallel loop match an existing plan.

The second part contains the data generated by the {\tt \bf op\_plan} routine:
\begin{itemize}
\item {\tt nthrcol${}^*$}:
an array with the number of thread colors for each block
\item {\tt thrcol${}^*$}:
an array with the thread color for each element of the primary set
\item {\tt offset${}^*$}:
an array with the primary set offset for the beginning of each block
\item {\tt ind\_maps${}^*$}:
a 2D array, the outer index over the indirect datasets, and the inner one giving the
local $\rightarrow$ global renumbering for the elements of the indirect set
\item {\tt ind\_offs${}^*$}:
a 2D array, the outer index over the indirect datasets, and the inner one giving the
starting offset into {\tt ind\_maps} for each block
\item {\tt ind\_sizes${}^*$}:
a 2D array, the outer index over the indirect datasets, and the inner one giving the
number of indirect elements for each block
\item {\tt maps${}^*$}:
a 2D array, the outer index over the datasets, and the inner one giving the indirect
mappings to local indices in shared memory
\item {\tt nelems${}^*$}:
an array with number of primary set elements in each block
\item {\tt ncolors}:
number of block colors
\item {\tt ncolblk}:
an array with number of blocks for each color
\item {\tt blkmap${}^*$}:
an array with mapping to blocks of each color
\item {\tt nshare}:
number of bytes of shared memory require to execute the plan
\end{itemize}

The data in the arrays marked ${}^*$ is initially generated on the host, then
transferred to the device, with only the array pointers being retained on the host.

\subsection{op\_plan\_check}

This routine checks the correctness of various aspects of a plan.  The checks
are performed automatically after the plan is constructed, depending on the
value of the diagnostics variable {\tt OP\_DIAGS}.

\newpage

\section{Data layout}

One key implementation choice is how to store datasets in which there are
multiple items for each set element.  For example, in the ``airfoil'' testcase
there are four flow variables for each cell.

The two alternatives are:
\begin{itemize}
\item SoA: for each component, store the data for all of the set elements
      as a contiguous block.
\item AoS: for each set element, store all of the components together
      as a small contiguous block;
\end{itemize}

The SoA format (which corresponds to what is referred to as
``a struct of arrays'') is what used to be used on CRAY vector computers
25 years ago.  It is ideally suited for systems which stream data into and
out of the processing unit, and a number of GPU programmers believe it is
the best approach for GPUs as well because it leads to natural coalescence
in memory transfers.

The AoS format (which corresponds to the alternative ``array of structs'')
is the preferred choice for systems with a cache hierarchy.
Assuming that all of the components are used within the computation, this
exploits spatial locality within a cache line; having loaded a cache line to
access one component, the other components are immediately available as well.

To understand the tradeoff between these two storage formats, we need to consider
cache line efficiency, the extent to which a computation uses all of the data in
a cache line.  In the ``airfoil'' testcase we need 4 floats per cell, and the
Fermi cache line size is 128 bytes, corresponding to 32 floats.  When data is
accessed indirectly, the SoA format can lead to a worst-case scenario in which
only 1/32 of the cache line is used, whereas with the AoS format the worst case
is that only 1/8 of the cache line is used.  Hence, in extreme cases with almost
random addressing, the AoS format could be 4 times more efficient than the SoA
format, and therefore require 1/4 of the data transfer from the graphics memory
to the GPU relative to SoA.  The savings could be even larger for applications
with more data per set element.

Another way of looking at this is that if there are $K$ data items per set
element, then the SoA format with a cache line of size $S$ will have the same
cache line efficiency as the AoS format with a cache line of size $KS$.
Hence, the AoS format increases the cache line size relative to the SoA format,
and therefore decreases the cache line efficiency.

For this reason, I have chosen to use the AoS format. The potential disadvantage
of this choice is that it destroys the natural memory coalescence of the SoA
format for direct loops.  However, the memory coalescence is still achieved in
OP2 through some additional programming effort.

\subsection{direct loops}

To achieve memory coalescence for direct loops we need to use shared memory.
As shown below in an example taken from {\tt save\_soln\_kernel.cu} in
the ``airfoil'' testcase, the threads in a CUDA warp use a coalesced memory
transfer to load into shared memory, and then copy it into local arrays which
the {\tt nvcc} compiler will map to registers.
\begin{verbatim}
  for (int m=0; m<4; m++)
    ((float *)arg_s)[tid+m*nelems] = arg0[tid+m*nelems+offset*4];

  for (int m=0; m<4; m++)
    arg0_l[m] = ((float *)arg_s)[m+tid*4];
\end{verbatim}
Here \verb!tid=threadIdx.x%OP_WARPSIZE! is the thread ID (modulo the warpsize)
and {\tt nelems} is the number of active threads in the warp; this is usually
equal to the warpsize, but if the set size is not an integer multiple of
the warpsize then there's a final warp in which some threads are not active.

Usually when using shared memory one has to use {\tt \_\_syncthreads}
to synchronize the actions of different warps.  However, this has a significant
impact on the overall performance, and is avoided by giving each warp its own
separate shared memory ``scratchpad'' to work in so there are no conflicts
between different warps.  This is accomplished by the declaration:
\begin{verbatim}
  char *arg_s = shared + offset_s*(threadIdx.x/OP_WARPSIZE);
\end{verbatim}
with the scratchpad size {\tt offset\_s} set to be large enough to handle
each of the datasets being loaded.

The same technique is used in reverse for coalesced writing back to the
graphics memory:
\begin{verbatim}
  for (int m=0; m<4; m++)
    ((float *)arg_s)[m+tid*4] = arg1_l[m];

  for (int m=0; m<4; m++)
    arg1[tid+m*nelems+offset*4] = ((float *)arg_s)[tid+m*nelems];
\end{verbatim}

Note: this treatment is only needed when there is more than one data element
per set element.  When there is only one, there is no need to pre-load it into
a register; the user kernel can load it directly from device memory in a
single coalesced transfer.

\newpage
\subsection{indirect loops}

In indirect loops the loading of data from graphics memory into shared
memory involves code of the following form, taken from
{\tt res\_calc\_kernel.cu} in the ``airfoil'' testcase.
\begin{verbatim}
  for (int n=threadIdx.x; n<ind_arg1_size*4; n+=blockDim.x)
    ind_arg1_s[n] = ind_arg1[n%4+ind_arg1_map[n/4]*4];
\end{verbatim}
Here \verb!ind_arg1_map[]! is an array giving the indices of the set
elements to be loaded.  This example has 4 components of data per set
element.  Hence the first 4 threads load the data for the first element,
the next 4 threads load the data for the second element, and so on.

Since the indices in \verb!ind_arg1_map[]! are sorted in ascending order,
the indices into the array \verb!ind_arg1! are also in ascending order.
This maximizes the degree of memory coalescence achievable. Each cache line
will be accessed by at most two consecutive warps, which minimizes the
reliance on L1 and L2 caches. This is important because in the future we
may want to turn off L1 caching (using the compiler flag {\tt -Xptxas -dlcm=cg};
see section G.4.2 in the CUDA Programming Guide) so that the L1 cache
memory is instead reserved exclusively for register spills.

Writing indirect datasets back to graphics memory is accomplished in a
similar manner.

\newpage

\section{op2.py preprocessor}

In this section I describe the code transformation performed by
{\bf op2.py} and discuss various aspects of the code which is generated.

\subsection{parsing the op\_par\_loop calls}

As explained in Section \ref{sec:indirect_datasets}, the pre-processor
finds each {\tt op\_par\_loop} call
and parses the arguments to identify the number of indirect
datasets which are used, and to define the {\tt inds[]} mapping
from the arguments to the indirect datasets.  It also identifies
how each of the arguments is being used (or ``accessed'').

If there are no indirect datasets the stub and kernel functions
which are generated are fairly simple.  The descriptions in the next
two sections are for the more interesting case in which there is
at least one indirect dataset.

\subsection{the CUDA stub routine}

The stub routine is the host routine which is called by the user's
main code.  If there are any local constants or global reduction
operations it starts by transferring this data to the GPU.

It then calls {\tt \bf op\_plan\_get} to get an existing plan or
generate a new one, passing into it the information about indirect
datasets which has been determined by the parser.

It then calls the kernel function to execute the plan.  This is
done within a loop over the different blocks colors, with an
implicit synchronization between each color to avoid any data
conflicts.

One of the kernel parameters is the amount of dynamic shared
memory; this comes from the maximum requirement determined
by {\tt \bf op\_plan\_core}.

Finally, for global reductions it fetches the output data back
to the CPU and does the final processing, combining the partial
results from each thread block.

%\newpage

\subsection{the CUDA kernel routine}

Most of the code in {\bf op2.py} is for the generation of the CUDA
kernel routines.  To understand this it is probably best to look
at an example of the generated code (e.g.~{\tt res\_kernel.cu})
while reading this description.

The key bits of code which are generated do the following:
\begin{itemize}
\item
declare correct number and type of input arguments, including
indirect datasets

\item
declare working variables, including local arrays which will probably
be held in registers (or in L1 cache on Fermi)

\item
get block ID using {\tt blkmap} mapping discussed in Section
\ref{sec:block_mapping}

\item
set the dynamic shared memory pointers for indirect datasets; see
the CUDA Programmer's Guide for more info on this

\item
zero out the memory for those being incremented, for Fermi,
copy the read-only indirect datasets into shared memory


\item
synchronize to ensure all shared memory data is ready before proceeding

\item
loop over all set elements in the block, and for each one
 \begin{itemize}
 \item zero out the local arrays for those being incremented
 \item execute the user function
 \item use thread coloring to increment the shared/global (Fermi/Kepler) memory data,
       with thread synchronization after each color
 \end{itemize}

\item
on Fermi cards, increment global storage of indirect datasets from shared memory

\item
complete any global reductions by updating the values in the main device
memory

\end{itemize}

%\newpage

Note: it is likely that the compiler will put small local arrays of
known size into registers.  This is why users should specify
{\tt op\_par\_loop} array dimensions as a literal constant.
(Currently, {\bf op2.py} doesn't handle dimensions which are set at
run-time, but that capability should be added in the future.)

There's one technical implementation detail which is very confusing.
In the code, the number of elements in the block is {\tt nelems}.
The variable {\tt nelems2} is equal to this value rounded up to the
nearest multiple of the number of threads in the thread block.
This ensures that every thread goes through the main loop the same
number of times.  This is important because the thread synchronization
command {\tt \_\_syncthreads} must be called by all threads.
The {\tt if} test within the loop prevents execution for elements
beyond the end of the block, and the default color is set to ensure
no participation in the colored increment.
%If we switch to atomic updates, the thread synchronization will
%no longer be needed, and so the main loop can simply go up to
%{\tt nelems}.

The generated code includes a number of pointers which are computed
by thread 0 and held in shared memory.  This is because the same values
are needed for all threads, and this minimizes register usage.

\subsection{the CUDA master kernels file}

The master kernels file is a single file which includes the kernel
files for each parallel loop along with some header files and the
declaration of the global constant variables which come from parsing
any {\tt op\_decl\_const} calls.

It has to be done this way in the current version of CUDA for the
constants to have global scope over all kernel files.


\subsection{the OpenMP files}

The OpenMP files generated by {\bf op2.py} execution pattern is very
similar to the CUDA one, but it is not necessary to use element coloring
within a mini-partition nor to launch a kernel, therefore the OpenMP
implementation uses a single function, computing indices and offsets
and then looping over the elements of the mini-parition, calling the user
kernels with pointers to ``global'' memory; no staging is used.

The other point worth discussing is the OpenMP implementation of
global reductions.  This is handled in a similar way to the CUDA
implementation.  Each OpenMP thread does a partial reduction using
thread-specific local variables.  These are then combined in a final
sequential step.

\subsection{the sequential files}

While OP2 can run sequentially (and with MPI) through the $op2\_seq.h$
header file, this is aimed at debugging and verification purposes; it uses
very generic logic and function pointers which add a significant oberhead
and prohibit some compiler optimisations. Therefore we generate
sequential stub files (referred to as genseq) that use information parsed by
 {\bf op2.py} to generate code specific to each loop; mappings for indirectly
 accessed data are only read once and then re-used, pointers are spelled
 out explicitly during the iteration through the elements in the set.

\subsection{the new source file}

The new source file generated by {\bf op2.py} for both CUDA and OpenMP
execution has only minor changes from the original source file:
\begin{itemize}
\item
new header file and declaration of function prototypes

\item
new names for each {\tt op\_par\_loop} call
\end{itemize}

\newpage

\section{Global reductions}

\subsection{Summation}

Each thread block has a separate entry in a device GPU array which is
initialized to zero.

Each thread sums its contributions, with the sum being initialized to zero.
The combined contributions from a single thread block are then combined
in a binary tree reduction process modelled on that the SDK ``reduction''
example, using shared memory.

The thread block sum is then added to the appropriate global entry for that
block.  Once the CUDA kernel call is complete the final block values are
transferred back to the CPU and added to the original starting value.

\subsection{Min/max reductions}

The treatment of min/max reductions is very similar. Each thread block
again has a separate entry in a device GPU array, but in this case
it is initialized to the initial CPU input value.

At the thread level the minimum/maximum is initialized using the current
global value for that block, and then updated using the thread's subsequent
contributions.  A binary tree approach combines these to form a thread
block minimum/maximum.

The thread block minimum/maximum values are used to update the global value.
Once the CUDA kernel call is complete the final block values are transferred
back to the CPU and combined to give the overall minimum/maximum.

\subsection{User-defined datatypes}

Summations should be fine, and min/max reductions also ought to work
provided the user has correctly overloaded the inequality operators
so that for all $a, b, c$,
\begin{itemize}
\item\vspace*{-0.1in} $a\!>\!b,\  b\!>\!c\ \Longrightarrow\ a\!>\!c$
           ~~ and ~~   $a\!<\!b,\ \ b\!<\!c\ \Longrightarrow\ a\!<\!c$
\item\vspace*{-0.1in}  either $a\!<\!b$, or $a\!>\!b$, or $a\!=\!b$
\end{itemize}

In the future, the MPI implementation could be handled by
implementing an all-to-all data exchange followed by local reduction,
with an option for standard MPI reductions to be used for standard datatypes.


%\newpage
%
%\section{User-defined datatypes}
%
%OP2 supports user-defined datatypes for datasets and constants.
%
%\subsection{Run-time type-checking}
%
%Run-time type-checking is implemented in a portable way in which the user
%is required to provide a {\tt type\_error} routine for each user-defined
%datatype.  The {\tt type\_error} routines for the standard datatypes are
%defined in {\tt op\_datatypes.h}.
%
%\subsection{Zero element for incrementing}
%
%When executing {\tt op\_par\_loop} for a case in which a dataset is
%incremented, the CUDA code which is generated by {\bf op2.m}
%initializes the increment to zero, calls the user-supplied kernel
%function to compute/add the new increment, and then uses coloring to
%add the increment to the dataset.
%
%With a user-defined datatype, the addition in the last step requires
%that the user has defined an overloaded addition operator.  The user
%also has to specify a zero element, called {\tt ZERO\_typename},
%which is used in the first step to initialize the increment correctly.
%
%The zero elements for the standard datatypes are defined in
%{\tt op\_datatypes.h}.  Note that {\tt typedef} is used to alias
%{\tt unsigned int}, {\tt long long} and {\tt unsigned long long} to
%{\tt uint}, {\tt ll} and {\tt ull}, respectively.  This is because
%{\tt typename} must not contain any spaces, otherwise there are problems
%with the compiler parsing of {\tt ZERO\_typename}.
%
%\subsection{Global reductions}
%
%Summations should be fine, and min/max reductions also ought to work
%provided the user has correctly overloaded the inequality operators
%so that for all $a, b, c$,
%\begin{itemize}
%\item\vspace*{-0.1in} $a\!>\!b,\  b\!>\!c\ \Longrightarrow\ a\!>\!c$
%           ~~ and ~~   $a\!<\!b,\ \ b\!<\!c\ \Longrightarrow\ a\!<\!c$
%\item\vspace*{-0.1in}  either $a\!<\!b$, or $a\!>\!b$, or $a\!=\!b$
%\end{itemize}
%
%In the future, the MPI implementation could be handled by
%implementing an all-to-all data exchange followed by local reduction,
%with an option for standard MPI reductions to be used for standard datatypes.
%
%\subsection{Alignment padding}
%
%When user-defined datatypes have elements with different sizes, the
%compiler automatically introduces padding to ensure correct data
%alignment, plus padding at the end, if necessary, to ensure that
%the next element in an array starts with the correct alignment.
%
%However, when using dynamic shared memory in CUDA, it is the
%programmer's responsibility to ensure the correct alignment, as
%explained in section B.2.3 of the CUDA Programming Guide, version 3.0.
%This requires the use of a {\tt ROUND\_UP} macro in {\bf op\_lib.cu}
%and the CUDA kernels generated by {\bf op2.m}.  This macro is
%modelled on the macro {\tt ALIGN\_UP} in the Programming Guide.
%
%Similar padding is used in assembling local constants and global
%reduction values into contiguous arrays for transfer to/from the GPU.



\newpage

\section{AVX implementation}

Experimental code generators can create code for AVX execution
by explicitly combining a number of set elements into vector operations,
first gathering data into vector registers, executing the user kernel with
vector types that have operators overloaded and then scattering the
data from vector registers. The code generated uses C++ classes to
hide the use of vector intrinsics, which are defined in $op2\_vechtor.h$

Since alignment is an important issue for directly accessed data, the loop
over set elements is broken into three parts; a scalar pre-sweep to get aligned
to 256/512 bits, the main vectorised loop and then a scalar post-sweep
to handle the elements left over.

There are some restrictions: branching can not be used in the user kernel
because we cannot overload that, instead $select()$ instructions can be
used.
\newpage

\section{Auto-tuning}

To optimize the run-time performance, I think it will be essential
to use auto-tuning techniques, building on ideas already used by
packages such as ATLAS and FFTW.  To emphasize this point, I don't
think I am talking about squeezing out an extra 10\% of performance;
I think it is more likely that auto-tuning will double the performance.


Here I list some of the choices to be optimized, and the tradeoffs
they involve:
\begin{itemize}
\item
Size of block partition

To maximize date reuse within each block, the natural choice is
to make the block partition as large as possible subject to the
constraint that it fits inside the shared memory.  However,
making it a bit smaller would allow multiple blocks to run
concurrently on the same multiprocessor, and the run-time
scheduler would then naturally overlap I/O for one block
with computation for another.

\item
Number of threads per block

One option is to have one thread per set element, giving the
run-time scheduler the maximum freedom.  However, this increases
the cost of thread synchronization, might run into difficulties
with the total register usage, and doesn't amortize startup
costs over multiple elements.

\end{itemize}


These choices exist for each parallel loop, but what is optimal
for one loop may not be optimal for another.
Optimization strategies which could be used include:
\begin{itemize}
\item
Brute force exhaustive search

%Specify a small set of possible values for the integer variables
%(block partition size and number of threads per block) and then
%systematically try every combination of choices and values.
%
%This would be particularly applicable if, perhaps through
%profiling, the user can identify the most important loop(s).

\item
Genetic algorithms (or other stochastic method)

%Brute force could work at the level of a single loop.  To handle
%lots of loops, one either uses brute force on each one, one at
%a time, or one could perhaps use a higher-dimensional method
%like GA to optimize them all at the same time.

\item
Run-time optimization

%In some cases, the optimal choice/value may depend on the specifics
%of the problem being solved, i.e.~may depend on the run-time data.
%If an application requires many timesteps or iterations, it could
%be feasible at the beginning to try different choices/values for
%different timesteps/iterations and monitor the execution times to
%decide on the best combination.

\end{itemize}


\newpage

\section{To do list}

\begin{itemize}
\item
Modify {\bf op2.m} to cope with more than one parallel loop using
the same user kernel function, and more than one call to set the
value of the same global constant.

\item
Add capability to handle dataset (and local and global constant) dimensions
which are not known until run-time.

\item
Add recursive geometric partitioning for local renumbering of sets and
mappings to improve data reuse.

\item
For some parallel loops (like the gradient calculation in HYDRA)
which are data-intensive and not compute-intensive, it might be better
for the thread coloring to also control the user kernel execution, so
that the indirect arrays held in shared memory can be directly incremented
rather than using local variables to hold the temporary increments.

\item
Within a thread block, could reorder elements to reduce the number of
different colors within each warp.  This would require the storage and
fetching of the permuted identity mapping

\item
Should use the {\tt restrict} qualifier where appropriate to enable
compiler optimization

\item
There is scope for relatively simple checkpointing. An {\bf op\_checkpoint}
instruction in the user code would trigger the saving of the state of all
of the OP2 datasets.  In addition, all global reduction values generated by
OP2 would be saved.  In the event of an application failure, the application
could be restarted and ``fast-forwarded'' to the last checkpoint at which
point the state would be restored.  The global reduction data is all that is
needed for the fast-forward to proceed correctly, as this is the only data
which is transferred from OP2 to the user's main application.

\item
Some of the block-invariant data which is currently initialised by thread 0
and put in shared memory could instead be pre-computing by the host, and
then accessed through the constant cache.

\item
For 3D structured grids, we use a block ``pencil'' with a 3-plane active
working set.  It's possible something similar could be done here, with each
thread block working on a sequence of mini-partitions, with some threads
loading in the data for the next one while others are doing the computation
for the current one.  Testing this idea could be a lot of work.


\end{itemize}

\end{document}

