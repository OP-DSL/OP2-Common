{# Template imports #}
{% from 'cpp/macros.j2' import op_opt, op_host_stub_args, op_host_set_mappings, op_host_vec_arg_arrays %}
{% from 'macros.j2' import comma %}

// user function
#include "../{{ parloop.kernelPath }}"

// host stub function
void op_par_loop_{{ parloop.name }}(
  char const *name,
  op_set set,
  {% for arg in parloop.unique %}
  op_arg arg{{ arg.i }}{{ comma(loop) }}
  {% endfor %}
) {

  {% for glb in parloop.globals %}
  {% if glb.acc != "OP_READ" %}
  {{ glb.typ }} *arg{{ glb.i }}h = ({{ glb.typ }} *)arg{{ glb.i }}.data;
  {% endif %}
  {% endfor %}

  {{ op_host_stub_args(parloop) }}

  // initialise timers
  double cpu_t1, cpu_t2, wall_t1, wall_t2;
  {% if opt.config['thread_timing'] %}
  op_timing_realloc_manytime({{ id }}, omp_get_max_threads());
  {% else %}
  op_timing_realloc({{ id }});
  {% endif %}
  OP_kernels[{{ id }}].name   = name;
  OP_kernels[{{ id }}].count += 1;
  op_timers_core(&cpu_t1, &wall_t1);
  {% if opt.config['thread_timing'] %}
  double non_thread_walltime = 0.0;
  {% endif %}

  {% if parloop.indirection %}
  int ninds = {{ parloop.indirectVars | length }};
  int inds[{{ parloop.nargs }}] = { {% for ind in parloop.indirectVarInds %} {{ ind }}{{ comma(loop) }} {% endfor %} };
  {% endif %}

  if (OP_diags>2) {
    printf(" kernel routine {{ 'with' if parloop.indirection else 'w/o' }} indirection: {{ parloop.name }}\n");
  }

  {% if parloop.indirection %}
  // Get plan
  #ifdef OP_PART_SIZE_{{ id }}
    int part_size = OP_PART_SIZE_{{ id }};
  #else
    int part_size = OP_part_size;
  #endif
  {% endif %}
  int set_size = op_mpi_halo_exchanges(set, nargs, args);

  {# Set number of threads #}
  {% if parloop.reduction or not parloop.indirection %}
  // Set number of threads
  #ifdef _OPENMP
    int nthreads = omp_get_max_threads();
  #else
    int nthreads = 1;
  #endif
  {% endif %}

  {# Create arrays for reduction #}
  {% if parloop.reduction %}
  // Allocate and initialise arrays for global reduction
  {% for glb in parloop.globals %}
  {% if glb.acc != "OP_READ" and glb.acc != "OP_WRITE" %}
  {{ glb.typ }} arg{{ glb.i }}_l[nthreads * 64];
  for(int thr = 0; thr < nthreads; thr++) {
    for(int d = 0; d < {{ glb.dim }}; d++) {
      {% if glb.acc == "OP_INC" %}
      arg{{ glb.i }}_l[d + thr * 64] = ZERO_{{ glb.typ }};
      {% else %}
      arg{{ glb.i }}_l[d + thr * 64] = arg{{ glb.i }}h[d];
      {% endif %}
    }
  }
  {% endif %}
  {% endfor %}
  {% endif %}

  if(set_size > 0) {
  {% if parloop.indirection %}
    {#
    #### Kernel call for indirect version ####
    #}
    op_plan *Plan = op_plan_get_stage_upload(name, set, part_size, nargs, args, ninds, inds, OP_STAGE_ALL, 0);
    // Execute plan
    int block_offset = 0;
    for(int col = 0; col < Plan->ncolors; col++) {
      if(col == Plan->ncolors_core)
        op_mpi_wait_all(nargs, args);

      int nblocks = Plan->ncolblk[col];
      {% if opt.config['thread_timing'] %}
      // Pause process timing and switch to per-thread timing:
      op_timers_core(&cpu_t2, &wall_t2);
      non_thread_walltime += wall_t2 - wall_t1;
      #pragma omp parallel
      {
      double thr_wall_t1, thr_wall_t2, thr_cpu_t1, thr_cpu_t2;
      op_timers_core(&thr_cpu_t1, &thr_wall_t1);

      int nthreads = omp_get_num_threads();
      int thr = omp_get_thread_num();
      int thr_start = (nblocks * thr) / nthreads;
      int thr_end = (nblocks * (thr+1)) / nthreads;
      if (thr_end > nblocks) thr_end = nblocks;
      for(int blockIdx = thr_start; blockIdx < thr_end; blockIdx++) {
      {% else %}
      #pragma omp parallel for
      for(int blockIdx = 0; blockIdx < nblocks; blockIdx++) {
      {% endif %}
        int blockId  = Plan->blkmap[blockIdx + block_offset];
        int nelem    = Plan->nelems[blockId];
        int offset_b = Plan->offset[blockId];
        for(int n = offset_b; n < offset_b + nelem; n++) {
          {# Declare and set mappings #}
          {% filter indent(width=10) %}
          {{ op_host_set_mappings(parloop) }}
          {% endfilter %}

          {# Create vec arguments #}
          {% filter indent(width=10) %}
          {{ op_host_vec_arg_arrays(parloop) }}
          {% endfilter %}

          {# Actual kernel call #}
          {{ parloop.name }}(
            {% for arg in parloop.unique %}
            {% if arg.direct %}
            &(({{ arg.typ }}*)arg{{ arg.datInd }}.data)[{{ arg.dim }} * n]{{ comma(loop) }}
            {% elif arg.indirect %}
            {% if arg.vec %}
            arg{{ arg.i }}_vec{{ comma(loop) }}
            {% elif not arg.vec %}
            &(({{ arg.typ }}*)arg{{ arg.datInd }}.data)[{{ arg.dim }} * map{{ arg.mapIdxInd }}idx]{{ comma(loop) }}
            {% endif %}
            {% elif arg.global_ %}
            {% if arg.acc != "OP_READ" and arg.acc != "OP_WRITE" %}
            &arg{{ arg.i }}_l[64 * omp_get_thread_num()]{{ comma(loop) }}
            {% else %}
            ({{ arg.typ }}*)arg{{ arg.i }}.data{{ comma(loop) }}
            {% endif %}
            {% endif %}
            {% endfor %}
          );
        }
        {% if opt.config['thread_timing'] %}
        }
        op_timers_core(&thr_cpu_t2, &thr_wall_t2);
        OP_kernels[{{ id }}].times[thr] += thr_wall_t2 - thr_wall_t1;
        {% endif %}
      }

      {% if parloop.reduction %}
      // Combine reduction data
      if(col == Plan->ncolors_owned - 1) {
        {% for glb in parloop.globals %}
        {% if glb.acc == "OP_INC" %}
        for(int thr = 0; thr < nthreads; thr++) {
          for(int d = 0; d < {{ glb.dim }}; d++) {
            arg{{ glb.i }}h[d] += arg{{ glb.i }}_l[d + thr * 64];
          }
        }
        {% elif glb.acc == "OP_MIN" %}
        for(int thr = 0; thr < nthreads; thr++) {
          for(int d = 0; d < {{ glb.dim }}; d++) {
            arg{{ glb.i }}h[d] = MIN(arg{{ glb.i }}h[d], arg{{ glb.i }}_l[d + thr * 64]);
          }
        }
        {% elif glb.acc == "OP_MAX" %}
        for(int thr = 0; thr < nthreads; thr++) {
          for(int d = 0; d < {{ glb.dim }}; d++) {
            arg{{ glb.i }}h[d] = MAX(arg{{ glb.i }}h[d], arg{{ glb.i }}_l[d + thr * 64]);
          }
        }
        {% elif glb.acc != "OP_READ" %}
        {{ parloop.raise_exception("Internal error: invalid reduction option (must be OP_INC, OP_MIN or OP_MAX)") }}
        {% endif %}
        {% endfor %}
      }
      {% endif %}
      {% if opt.config['thread_timing'] %}
      // Revert to process-level timing:
      op_timers_core(&cpu_t1, &wall_t1);
      {% endif %}
      block_offset += nblocks;
    }
    OP_kernels[{{ id }}].transfer  += Plan->transfer;
    OP_kernels[{{ id }}].transfer2 += Plan->transfer2;
  {% else %}
    {#
    #### Kernel call for direct version ####
    #}
    // Execute plan
    {% if opt.config['thread_timing'] %}
    // Pause process timing, and switch to per-thread timing:
    op_timers_core(&cpu_t2, &wall_t2);
    non_thread_walltime += wall_t2 - wall_t1;
    {% endif %}
    #pragma omp parallel for
    for(int thr = 0; thr < nthreads; thr++) {
      {% if opt.config['thread_timing'] %}
      double thr_wall_t1, thr_wall_t2, thr_cpu_t1, thr_cpu_t2;
      op_timers_core(&thr_cpu_t1, &thr_wall_t1);
      {% endif %}
      int start  = (set->size * thr) / nthreads;
      int finish = (set->size * (thr + 1)) / nthreads;
      for(int n = start; n < finish; n++) {
        {# Actual kernel call #}
        {{ parloop.name }}(
          {% for arg in parloop.unique %}
          {% if arg.direct %}
          &(({{ arg.typ }}*)arg{{ arg.datInd }}.data)[{{ arg.dim }} * n]{{ comma(loop) }}
          {% elif arg.global_ %}
          {% if arg.acc != "OP_READ" and arg.acc != "OP_WRITE" %}
          &arg{{ arg.i }}_l[64 * omp_get_thread_num()]{{ comma(loop) }}
          {% else %}
          ({{ arg.typ }}*)arg{{ arg.i }}.data{{ comma(loop) }}
          {% endif %}
          {% endif %}
          {% endfor %}
        );
      }
      {% if opt.config['thread_timing'] %}
      op_timers_core(&thr_cpu_t2, &thr_wall_t2);
      OP_kernels[{{ id }}].times[thr] += thr_wall_t2 - thr_wall_t1;
      {% endif %}
    }
    {% if opt.config['thread_timing'] %}
    // OpenMP block complete, so switch back to process timing:
    op_timers_core(&cpu_t1, &wall_t1);
    {% endif %}
  {% endif %}
  }

  {# Account for a zero set size #}
  {% if parloop.indirection %}
  if(set_size == 0 || set_size == set->core_size)
    op_mpi_wait_all(nargs, args);
  {% endif %}

  {# Combine reduction data for direct loops #}
  // Combine reduction data
  {% if parloop.direct %}
  {% for glb in parloop.globals %}
  {% if glb.acc == "OP_INC" %}
  for(int thr = 0; thr < nthreads; thr++) {
    for(int d = 0; d < {{ glb.dim }}; d++) {
      arg{{ glb.i }}h[d] += arg{{ glb.i }}_l[d + thr * 64];
    }
  }
  {% elif glb.acc == "OP_MIN" %}
  for(int thr = 0; thr < nthreads; thr++) {
    for(int d = 0; d < {{ glb.dim }}; d++) {
      arg{{ glb.i }}h[d] = MIN(arg{{ glb.i }}h[d], arg{{ glb.i }}_l[d + thr * 64]);
    }
  }
  {% elif glb.acc == "OP_MAX" %}
  for(int thr = 0; thr < nthreads; thr++) {
    for(int d = 0; d < {{ glb.dim }}; d++) {
      arg{{ glb.i }}h[d] = MAX(arg{{ glb.i }}h[d], arg{{ glb.i }}_l[d + thr * 64]);
    }
  }
  {% endif %}
  {% endfor %}
  {% endif %}

  {# Combine reduction data over MPI for all types of loops #}
  {% for glb in parloop.globals %}
  {% if glb.acc != "OP_READ" %}
  op_mpi_reduce(&arg{{ glb.i }}, arg{{ glb.i }}h);
  {% endif %}
  {% endfor %}
  op_mpi_set_dirtybit(nargs, args);

  // Update kernel record
  op_timers_core(&cpu_t2, &wall_t2);
  {% if opt.config['thread_timing'] %}
  non_thread_walltime += wall_t2 - wall_t1;
  OP_kernels[{{ id }}].times[0] += non_thread_walltime;
  {% else %}
  OP_kernels[{{ id }}].time += wall_t2 - wall_t1;
  {% endif %}
  {% if parloop.direct %}
  {% for arg in parloop.args %}
  {%- call op_opt(arg) %}
  {% if not arg.global_ %}
  OP_kernels[{{ id }}].transfer += (float)set->size * arg{{ arg.i }}.size{{ '' if arg.acc == "OP_READ" else ' * 2.0f' }};
  {% endif %}
  {%- endcall %}
  {% endfor %}
  {% endif %}
}
